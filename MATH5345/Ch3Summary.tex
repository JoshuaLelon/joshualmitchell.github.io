% Thank you Josh Davis for this template!
% https://github.com/jdavis/latex-homework-template/blob/master/homework.tex

\documentclass[8pt]{extarticle}

\newcommand{\hmwkTitle}{Chapter 3 Summary}

% \input{ShortcutsStatistics}

% ----------

% Packages

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}
\usepackage{enumitem}
\usepackage{chngcntr}
\usepackage{blkarray}
\usepackage{extsizes}

% Libraries

\usetikzlibrary{automata, positioning, arrows}

%
% Basic Document Settings
%

\topmargin=-0.85in
\evensidemargin=-0.75in
\oddsidemargin=-0.75in
\textwidth=8in
\textheight=10.0in
\headsep=0.1in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{}
\rhead{\hmwkClass\ (\hmwkClassInstructor): \hmwkTitle}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.2pt}
\renewcommand\footrulewidth{0.2pt}

\setlength\parindent{0pt}
\setcounter{secnumdepth}{0}

\newcommand{\hmwkClass}{MATH 5345 / Regression Analysis}        % Class
\newcommand{\hmwkClassInstructor}{Dr. Sun}           % Instructor
\newcommand{\hmwkAuthorName}{\textbf{Joshua Mitchell}} % Author

%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small\vspace{0.1in}\large{\textit{\hmwkClassInstructor}}
    \vspace{3in}
}

\author{\hmwkAuthorName}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

%
% Various Helper Commands
%

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}


% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Formatting commands:

\newcommand{\mt}[1]{\ensuremath{#1}}
\newcommand{\nm}[1]{\textrm{#1}}

\newcommand\bsc[2][\DefaultOpt]{%
  \def\DefaultOpt{#2}%
  \section[#1]{#2}%
}
\newcommand\ssc[2][\DefaultOpt]{%
  \def\DefaultOpt{#2}%
  \subsection[#1]{#2}%
}
\newcommand{\bgpf}{\begin{proof} $ $\newline}

\newcommand{\bgeq}{\begin{equation*}}
\newcommand{\eeq}{\end{equation*}}	

\newcommand{\balist}{\begin{enumerate}[label=\alph*.]}
\newcommand{\elist}{\end{enumerate}}

\newcommand{\bilist}{\begin{enumerate}[label=\roman*)]}	

\newcommand{\bgsp}{\begin{split}}
% \newcommand{\esp}{\end{split}} % doesn't work for some reason.

\newcommand\prs[1]{~~~\textbf{(#1)}}

\newcommand{\lt}[1]{\textbf{Let: } #1}
     							   %  if you're setting it to be true
\newcommand{\supp}[1]{\textbf{Suppose: } #1}
     							   %  Suppose (if it'll end up false)
\newcommand{\wts}[1]{\textbf{Want to show: } #1}
     							   %  Want to show
\newcommand{\as}[1]{\textbf{Assume: } #1}
     							   %  if you think it follows from truth
\newcommand{\bpth}[1]{\textbf{(#1)}}

\newcommand{\step}[2]{\begin{equation}\tag{#2}#1\end{equation}}
\newcommand{\epf}{\end{proof}}

\newcommand{\dbs}[3]{\mt{#1_{#2_#3}}}

\newcommand{\sidenote}[1]{-----------------------------------------------------------------Side Note----------------------------------------------------------------
#1 \

---------------------------------------------------------------------------------------------------------------------------------------------}

% Analysis / Logical commands:

\newcommand{\br}{\mt{\mathbb{R}} }       % |R
\newcommand{\bq}{\mt{\mathbb{Q}} }       % |Q
\newcommand{\bn}{\mt{\mathbb{N}} }       % |N
\newcommand{\bc}{\mt{\mathbb{C}} }       % |C
\newcommand{\bz}{\mt{\mathbb{Z}} }       % |Z

\newcommand{\ep}{\mt{\epsilon} }         % epsilon
\newcommand{\fa}{\mt{\forall} }          % for all
\newcommand{\afa}{\mt{\alpha} }
\newcommand{\bta}{\mt{\beta} }
\newcommand{\mem}{\mt{\in} }
\newcommand{\exs}{\mt{\exists} }

\newcommand{\es}{\mt{\emptyset} }        % empty set
\newcommand{\sbs}{\mt{\subset} }         % subset of
\newcommand{\fs}[2]{\{\uw{#1}{1}, \uw{#1}{2}, ... \uw{#1}{#2}\}}

\newcommand{\lra}{ \mt{\longrightarrow} } % implies ----->
\newcommand{\rar}{ \mt{\Rightarrow} }     % implies -->

\newcommand{\lla}{ \mt{\longleftarrow} }  % implies <-----
\newcommand{\lar}{ \mt{\Leftarrow} }      % implies <--

\newcommand{\av}[1]{\mt{|}#1\mt{|}}  % absolute value

\newcommand{\prn}[1]{(#1)}
\newcommand{\bk}[1]{\{#1\}}

\newcommand{\ps}{\mt{\operatorname{+}} }
\newcommand{\ms}{\mt{\operatorname{-}} }

\newcommand{\ls}{\mt{\operatorname{<}} }
\newcommand{\gr}{\mt{\operatorname{>}} }

\newcommand{\lse}{\mt{\operatorname{\leq}} }
\newcommand{\gre}{\mt{\operatorname{\geq}} }

\newcommand{\eql}{ \mt{\operatorname{=}} }

\newcommand{\pr}{\mt{^\prime}} 		   % prime (i.e. R')
\newcommand{\uw}[2]{#1\mt{_{#2}}}
\newcommand{\uf}[2]{#1\mt{^{#2}}}
\newcommand{\frc}[2]{\mt{\frac{#1}{#2}}}
\newcommand{\lmti}[1]{\mt{\displaystyle{\lim_{#1 \to \infty}}}}
\newcommand{\limt}[2]{\mt{\displaystyle{\lim_{#1 \to #2}}}}

\newcommand{\bnm}[2]{\mt{#1\setminus{#2}}}
\newcommand{\bnt}[2]{\mt{\textrm{#1}\setminus{\textrm{#2}}}}
\newcommand{\bi}{\bnm{\mathbb{R}}{\mathbb{Q}}}

\newcommand{\urng}[2]{\mt{\bigcup_{#1}^{#2}}}
\newcommand{\nrng}[2]{\mt{\bigcap_{#1}^{#2}}}
\newcommand{\nck}[2]{\mt{{#1 \choose #2}}}

\newcommand{\nbho}[3]{\textrm{N(}#1, #2\textrm{) }\cap \textrm{ #3} \neq \emptyset}
     							   %  N(x, eps) intersect S \= emptyset
\newcommand{\nbhe}[3]{\textrm{N(}#1, #2\textrm{) }\cap \textrm{ #3} = \emptyset}
     							   %  N(x, eps) intersect S  = emptyset
\newcommand{\dnbho}[3]{\textrm{N*(}#1, #2\textrm{) }\cap \textrm{ #3} \neq \emptyset}
     							   %  N*(x, eps) intersect S \= emptyset
\newcommand{\dnbhe}[3]{\textrm{N*(}#1, #2\textrm{) }\cap \textrm{ #3} = \emptyset}
     							   %  N*(x, eps) intersect S = emptyset
     							   
\newcommand{\eqn}[1]{\[#1\]}
\newcommand{\splt}[1]{\begin{split}#1\end{split}}

\newcommand{\txt}[1]{\text{#1}} % Not new command, but remember \text for text in eqns
\newcommand{\tl}{\mt{\thicksim}}
\newcommand{\mn}[1]{\mt{\overline{#1}}}
\newcommand{\sg}{\mt{\sigma}}
\newcommand{\ssq}{\mt{\sigma^2}}	

\newcommand{\bh}[1]{\mathbf{\hat{\text{$#1$}}}}
\newcommand{\bth}{\mt{\bh{\beta}}}
\newcommand{\yh}{\mt{\bh{Y}}}

\newcommand{\exv}[1]{\txt{E[}#1\txt{]}}
\newcommand{\vrn}[1]{V[#1]}

\newcommand{\gv}{ \mt{|} }

\newcommand{\cov}[2]{\txt{Cov(#1, #2)}}

\newcommand{\img}[1]{
\begin{figure}[h]
  \includegraphics[width=0.5\linewidth]{#1}
\end{figure}
}
\newcommand{\simg}[1]{
  \includegraphics[width=0.35\linewidth]{#1}
}

\newcommand\tab[1][1cm]{\hspace*{#1}}	
\newcommand{\sumin}[1]{\mt{\sum_{i = 1}^n #1}}	

\newcommand{\brm}[1]{\begin{pmatrix} #1 \end{pmatrix}}

\newcommand{\inm}[1]{\mt{\left\[ \begin{smallmatrix} #1 \end{smallmatrix} \right\]}}

\newcommand{\lbm}[4]{
	  \begin{blockarray}{#1} % a c for every row, plus the c-labels
        #2 \\ % & c-label1 & c-label2 & c-label3...
      \begin{block}{c(#3)} % a c for every column only
        #4 % r-label1 & data & data & data ... \\
           % r-label2 & data & data & data ... \\
           % ...
           % r-labeln & data & data & data ... \\
      \end{block}
    \end{blockarray}
}

% Example:
% \[\lbm{ccccc}{& H & y & d}{cccc}{H & 4 & 4 & 4 \\
% Y & 3 & 3 & 3 \\
% D & 2 & 2 & 2 \\
% D & 2 & 2 & 2 \\}\]

\newcommand{\unds}[2]{\mt{\underset{#1}{#2}}} % stuff underneath!	 
% ----------

\begin{document}

\ssc{Chapter 0: Review}{

}
\ssc{Chapter 2: Simple Linear Regression}{
\textbf{\exv{y$|$x}}$=\mu_{y|x}=$\exv{$\beta_0+\beta_1x+\epsilon$}$=\beta_0+\beta_1x$ \tab \textbf{\vrn{y$|$x}}$=\ssq_{y|x}=$\textbf{\vrn{$\beta_0+\beta_1x+\epsilon$}}$=\ssq$\tab \textbf{\uw{\bth}{0}}$=\bar y - \bth_1\bar x$ \tab \textbf{\uw{\bth}{1}}$=\frac{S_{xy}}{S_{xx}}=\frac{\sum_{i = 1}^n(x_i - \bar x)y_i}{\sum_{i = 1}^n(x_i - \bar x)^2}$

\textbf{\exv{\bth$_1$}}$=\sum_{i = 1}^n c_i\exv{y_i}=\beta_0\sum_{i = 1}^nc_i + \beta_0\sum_{i = 1}^nc_ix_i=\beta_1$\tab \textbf{\vrn{\bth$_1$}}$=\sum_{i = 1}^nc_i^2(\ssq)=\ssq \frac{\sum_{i = 1}^n(x_i - \bar x)^2}{S_{xx}^2}=\frac{\ssq}{S_{xx}}$

\textbf{\exv{\bth$_0$}}$=\beta_0$\tab \textbf{\vrn{\bth$_0$}}$=\ssq(\frac{1}{n} + \frac{\bar x^2}{S_{xx}})=\vrn{\bar y - \beta_1\bar x)}=\vrn{\bar y} + x^2 \vrn{\bth_1} - cov(\bar y, \bth_1)$\tab \textbf{\uw{c}{i}}$=\frac{x - \bar x}{S_{xx}}$

\textbf{SS$_{res}$}$=\sum_{i = 1}^n(y_i - \hat y_i)^2=\sum_{i = 1}^n \epsilon_i^2$\tab \textbf{SS$_T$}$=\sum_{i = 1}^n y_i^2 - n{\bar y}^2, n-1$ df \tab \textbf{SS$_{Reg}$}$=\bth_1S_{xy}$, if df\eql1, then $=MS_{Res}$

\textbf{MS$_{res}$}$=\ssq=\frac{SS_{res}}{n - 2}$

\ssc{Hypothesis Testing (Regression)}{

\textbf{Reject \uw{H}{0} if} \av{\uw{t}{0}} \gre \uw{t}{\frc{\afa}{2}, n \ms 2} where $t_0 = \frac{\bth_1 - \bta_{10}}{\sqrt{\frac{MS_{res}}{S_{xx}}}}$ \tab Failing to reject \uw{H}{0}: $\bta_i = 0$ implies no rlshp between x and y. \tab \exv{$y_i$} \eql \uw{\bta}{1}x \ps \uw{\bta}{0}

$F_0 = \frac{MS_{Reg}}{MS_{res}} = t_0^2$ \tab Reject if $F_0 \gr F_\alpha, 1, n-1$ \tab CI: $\bth_1 - t_{\frac{\alpha}{2}, n - 2}se(\bth_{10}) < \bth_{10} < \bth_1 + t_{\frac{\alpha}{2}, n - 2}se(\bth_{10})$ \tab se($\bth_1$)$=\sqrt{\frac{MS_{res}}{S_{xx}}}$, se($\bth_0$)$=\sqrt{\vrn{\bth_0}}$

\uf{R}{2}$=1 - \frac{SS_{res}}{SS_T} = \frac{SS_{Reg}}{SS_T}$\tab R$^2_{adj} = 1 - \frac{SS_{res}(n - 1)}{SS_T(n - k - 1}$ (penalizes you for adding nonsignificant terms to the model)
}
}
\ssc{Chapter 3: Multiple Linear Regression}{

\mt{\unds{n\times1}{y} = \unds{n\times p}{x}\times\unds{p\times1}{\bta} + \unds{n\times1}{\epsilon}}
where p \eql k \ps 1, p is the total number of betas (or parameters), k is the number of regressor variables.

\ep \tl N(0, \ssq I) where I is the identity matrix whatever size\tab \exv{y} \eql x\bta \tab \vrn{y} \eql \vrn{$\epsilon$} \eql \ssq I \tab y \tl N(x\bta, \ssq I)

}
\ssc{Least Square Estimate for \bta and \ssq}{
\mt{S(\beta)=\sum_{i = 1}^n \epsilon^2 = \epsilon\pr \epsilon = (y - x\beta)\pr(y - x\beta) = y\pr y - 2\beta\pr x\pr y + \beta\pr x\pr x\beta}

\mt{\bth = (x\pr x)^{-1}x\pr y}\tab \exv{\bth} \eql \mt{\exv{(x\pr x)^{-1}x\pr y}}\eql \mt{\exv{(x\pr x)^{-1}x\pr(x\bta + \ep)}} \eql \bta \tab \vrn{\bth} \eql (x\pr x)$^{-1}$\ssq \eql c\ssq \tab \vrn{\bth$_j$} \eql c$_{jj}$\ssq \tab \exv{\bta$_j$} \eql \uw{\bta}{j} 

\uw{\bth}{j} \tl N(\uw{\bta}{j}, \uw{c}{jj}\ssq) \mt{\hat y = x\hat\beta= (x(x\pr x)^{-1}x\pr) y = \textbf{H}y} \tab \exv{$\bh{y}$} \eql \exv{x\bth} \eql x\bta \tab \vrn{$\bh{y}$} \eql \vrn{x\bth} \eql x\vrn{\bth}x\pr \eql x(x\pr x)$^{-1}$x\pr\ssq \eql H\ssq

$\bh{y}$ \tl N(x\bta, H\ssq) \tab $\bh{y}_j$ \tl N(\uw{x}{j}\bta, \uw{h}{jj}\ssq), where \mt{h_{jj} = x_j\pr(x\pr x)^{-1}x_j} \tab \mt{x_j = [x_{j0}, x_{j1}, ... x_{jk}]} and \tab \mt{\hat\epsilon = y - \hat y = y - Hy = (I - H)y}

\mt{\hat\ssq (estimator) = \frac{SS_{res}}{n - p} = MS_{res}} where p \eql k \ps 1 \eql the number of parameters (i.e. \bta's: \uw{\bta}{0}, \uw{\bta}{1}, ... \uw{\bta}{k}) \tab
\textbf{Cov[\bth]}$=\ssq(X\pr X)^{-1}$ (cov matrix c)

\mt{SS_{res} (\textbf{n - p}) = (y - x\bth)\pr(y - x\bth) = y\pr y - 2\bth\pr X\pr y + \bth\pr x\pr x \bth = y\pr y - \bth\pr x\pr y} \tab 
\mt{SS_{Reg} (\textbf{k}) =\bth\pr x\pr y - \frac{(\sum_{i = 1}^n y_i)^2}{n}} \tab
\mt{SS_T (\textbf{n - 1}) = y\pr y - \frac{(\sum_{i = 1}^n y_i)^2}{n}}

\mt{MS_{res}=\frac{SS_{res}}{n - k - 1}} \tab
\mt{MS_{Reg}=\frac{SS_{Reg}}{k}} \tab 
\mt{MS_T = \frac{SS_T}{n - 1}} \tab

If $\frac{SS_{res}}{\ssq}$\tl$\chi^2_{n - k - 1}$ and SS$_{res}$, SS$_{Reg}$ are indep, then \mt{F_0 = \frac{\frac{SS_{Reg}}{k}}{\frac{SS_{Res}}{n - k - 1}} = \frac{MS_{Reg}}{MS_{res}}} \textbf{F statistic} \tab 
We reject \uw{H}{0} if \uw{F}{0} \gr \uw{F}{\afa, k, n - k - 1}

error \eql (I \ms H)y \eql (I \ms H)\ep \tab \exv{MS$_{Res}$} \eql \ssq \tab \exv{MS$_{Reg}$} \eql \ssq \mt{+ \frac{\bta^{*'}x\pr_c x_c \bta^*}{k\ssq}} where \mt{\beta^* = (\beta_1, \beta_2, ... \beta_k)} and \mt{x_c} is the center



\textbf{Testing Individual Coefficients (Partial Test):} If \uw{H}{0}: \uw{\bta}{j} \eql 0 is not rejected then delete it: \mt{t_0 = \frac{\bth_j}{\sqrt{\ssq c_{jj}}} = \frac{\bth_j}{se(\bth_j)}} \tab
reject if \av{\uw{t}{0}} \gr \uw{t}{\frc{\afa}{2}, n - k - 1}
}
\ssc{Confidence Intervals}{

\textbf{\ssq known}: \mt{\hat\beta_j \tl N(\beta_j, c_{jj}\ssq) \lra \frac{\hat\beta_j - \beta_j}{\sqrt{c_{jj}\ssq}} \tl N(0, 1)}
or, if variance is unknown,
\mt{\hat\beta_j \tl N(\beta_j, c_{jj}MS_{res}) \lra \frac{\hat\beta_j - \beta_j}{\sqrt{c_{jj}MS_{res}}} \textrm{or \tab} \frac{\hat\beta_j - \beta_j}{se(\hat\beta_j)} \tl t_{n - p}}

Then the variance estimator is \mt{\hat\ssq = MS_{res} = \frac{SS_{res}}{n - p} \tl \chi^2_{n - p}} \tab So, the (1 \ms \afa) \textbf{confidence interval} for \uw{\bta}{j} is \mt{\hat\beta_j \pm \uw{t}{\frac{\afa}{2}, n - p}se(\hat\beta_j)}

\mt{100(1-\alpha)\%} for \ssq: \mt{\frac{(n - 2)MS_{res}}{\chi^2_{\frac{\alpha}{2}, n - 2}} \lse \ssq \lse \frac{(n - 2)MS_{res}}{\chi^2_{\frac{1 - \alpha}{2}, n - 2}}} \tab
 $\hat y_j$ \tl N(\uw{x}{j}\bta, \uw{h}{jj}\ssq), so \tab \mt{\frac{\hat y_j - x_j\beta}{\sqrt{h_{jj}\ssq}} \tl N(0, 1) \textrm{\tab} \frac{\hat y_j - x_j\beta}{\sqrt{h_{jj}MS_{res}}} \tl t_{n - p}} \uw{MS}{res} ests \ssq 

A 1 \ms \afa confidence interval for \exv{\uw{y}{0}$|$\uw{x}{0}} is \mt{\hat y_0 \pm t_{\frac{\alpha}{2}, n - p}\sqrt{\uw{x}{0}\pr (x\pr x)^{-1}x_0\hat\ssq}} 
or 
\mt{\hat y_0 \pm t_{\frac{\alpha}{2}, n - p}\sqrt{\uw{x}{0}\pr (x\pr x)^{-1}x_0MS_{res}}}
}

\ssc{Chapter 4: Model Testing}{
Properties of residuals: mean 0, \mt{MS_{res} = \sum_{i = 1}^n \frac{(\epsilon_i - \bar\epsilon)^2}{n - p} = \sum_{i = 1}^n \frac{\epsilon^2_i}{n - p} = \frac{SS_{res}}{n - p}}

\textbf{Scaling Residuals:} Standardized Residuals: \mt{d_i = \frac{\epsilon_i}{\sqrt{MS_{res}}}} Studentized: \mt{r_i = \frac{\epsilon_i}{\sqrt{MS_{res}(1 - h_{ii})}}}, \textbf{\vrn{\uw{\ep}{i}}} \eql \ssq(1 \ms \uw{h}{ii}), \textbf{cov(\uw{\ep}{i}, \uw{\ep}{j})} \eql $-\ssq h_{ij}$
}

Other model testing: plot \uw{x}{i} and \uw{x}{j}: linear rln means high corr.

Formal test for lack of fit: Assuming everything is tested and ideal, to test for linearity, we use: \mt{SS_{res} = SS_{PE} + SS_{LOF}}

\uw{F}{0} \eql \mt{\frac{SS_{LOF} / (m - 2)}{SS_{PE} / (n - m)}} \eql \mt{\frac{MS_{LOF}}{MS_{PE}}} \tab
\exv{\mt{MS_{LOF}}} \eql \mt{} \exv{\mt{MS_{PE}}} \eql \ssq, where m is num regressors, n is num samples
\tab \vrn{$\bar y$} \eql \mt{\frac{p\ssq}{n}} (indpure e)
\end{document}