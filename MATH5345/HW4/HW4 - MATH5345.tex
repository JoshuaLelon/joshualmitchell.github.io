% Thank you Josh Davis for this template!
% https://github.com/jdavis/latex-homework-template/blob/master/homework.tex

\documentclass{article}

\newcommand{\hmwkTitle}{HW\ \#4}

% \input{ShortcutsStatistics}

% ----------

% Packages

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}
\usepackage{enumitem}
\usepackage{chngcntr}

% Libraries

\usetikzlibrary{automata, positioning, arrows}

%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{}
\rhead{\hmwkClass\ (\hmwkClassInstructor): \hmwkTitle}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}
\setcounter{secnumdepth}{0}

\newcommand{\hmwkClass}{MATH 5345 / Regression Analysis}        % Class
\newcommand{\hmwkClassInstructor}{Dr. Sun}           % Instructor
\newcommand{\hmwkAuthorName}{\textbf{Joshua Mitchell}} % Author

%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small\vspace{0.1in}\large{\textit{\hmwkClassInstructor}}
    \vspace{3in}
}

\author{\hmwkAuthorName}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

%
% Various Helper Commands
%

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}


% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Formatting commands:

\newcommand{\mt}[1]{\ensuremath{#1}}
\newcommand{\nm}[1]{\textrm{#1}}

\newcommand\bsc[2][\DefaultOpt]{%
  \def\DefaultOpt{#2}%
  \section[#1]{#2}%
}
\newcommand\ssc[2][\DefaultOpt]{%
  \def\DefaultOpt{#2}%
  \subsection[#1]{#2}%
}
\newcommand{\bgpf}{\begin{proof} $ $\newline}

\newcommand{\bgeq}{\begin{equation*}}
\newcommand{\eeq}{\end{equation*}}	

\newcommand{\balist}{\begin{enumerate}[label=\alph*.]}
\newcommand{\elist}{\end{enumerate}}

\newcommand{\bilist}{\begin{enumerate}[label=\roman*)]}	

\newcommand{\bgsp}{\begin{split}}
% \newcommand{\esp}{\end{split}} % doesn't work for some reason.

\newcommand\prs[1]{~~~\textbf{(#1)}}

\newcommand{\lt}[1]{\textbf{Let: } #1}
     							   %  if you're setting it to be true
\newcommand{\supp}[1]{\textbf{Suppose: } #1}
     							   %  Suppose (if it'll end up false)
\newcommand{\wts}[1]{\textbf{Want to show: } #1}
     							   %  Want to show
\newcommand{\as}[1]{\textbf{Assume: } #1}
     							   %  if you think it follows from truth
\newcommand{\bpth}[1]{\textbf{(#1)}}

\newcommand{\step}[2]{\begin{equation}\tag{#2}#1\end{equation}}
\newcommand{\epf}{\end{proof}}

\newcommand{\dbs}[3]{\mt{#1_{#2_#3}}}

\newcommand{\sidenote}[1]{-----------------------------------------------------------------Side Note----------------------------------------------------------------
#1 \

---------------------------------------------------------------------------------------------------------------------------------------------}

% Analysis / Logical commands:

\newcommand{\br}{\mt{\mathbb{R}} }       % |R
\newcommand{\bq}{\mt{\mathbb{Q}} }       % |Q
\newcommand{\bn}{\mt{\mathbb{N}} }       % |N
\newcommand{\bc}{\mt{\mathbb{C}} }       % |C
\newcommand{\bz}{\mt{\mathbb{Z}} }       % |Z

\newcommand{\ep}{\mt{\epsilon} }         % epsilon
\newcommand{\fa}{\mt{\forall} }          % for all
\newcommand{\afa}{\mt{\alpha} }
\newcommand{\bta}{\mt{\beta} }
\newcommand{\mem}{\mt{\in} }
\newcommand{\exs}{\mt{\exists} }

\newcommand{\es}{\mt{\emptyset} }        % empty set
\newcommand{\sbs}{\mt{\subset} }         % subset of
\newcommand{\fs}[2]{\{\uw{#1}{1}, \uw{#1}{2}, ... \uw{#1}{#2}\}}

\newcommand{\lra}{ \mt{\longrightarrow} } % implies ----->
\newcommand{\rar}{ \mt{\Rightarrow} }     % implies -->

\newcommand{\lla}{ \mt{\longleftarrow} }  % implies <-----
\newcommand{\lar}{ \mt{\Leftarrow} }      % implies <--

\newcommand{\av}[1]{\mt{|}#1\mt{|}}  % absolute value

\newcommand{\prn}[1]{(#1)}
\newcommand{\bk}[1]{\{#1\}}

\newcommand{\ps}{\mt{\operatorname{+}} }
\newcommand{\ms}{\mt{\operatorname{-}} }

\newcommand{\ls}{\mt{\operatorname{<}} }
\newcommand{\gr}{\mt{\operatorname{>}} }

\newcommand{\lse}{\mt{\operatorname{\leq}} }
\newcommand{\gre}{\mt{\operatorname{\geq}} }

\newcommand{\eql}{ \mt{\operatorname{=}} }

\newcommand{\pr}{\mt{^\prime}} 		   % prime (i.e. R')
\newcommand{\uw}[2]{#1\mt{_{#2}}}
\newcommand{\uf}[2]{#1\mt{^{#2}}}
\newcommand{\frc}[2]{\mt{\frac{#1}{#2}}}
\newcommand{\lmti}[1]{\mt{\displaystyle{\lim_{#1 \to \infty}}}}
\newcommand{\limt}[2]{\mt{\displaystyle{\lim_{#1 \to #2}}}}

\newcommand{\bnm}[2]{\mt{#1\setminus{#2}}}
\newcommand{\bnt}[2]{\mt{\textrm{#1}\setminus{\textrm{#2}}}}
\newcommand{\bi}{\bnm{\mathbb{R}}{\mathbb{Q}}}

\newcommand{\urng}[2]{\mt{\bigcup_{#1}^{#2}}}
\newcommand{\nrng}[2]{\mt{\bigcap_{#1}^{#2}}}
\newcommand{\nck}[2]{\mt{{#1 \choose #2}}}

\newcommand{\nbho}[3]{\textrm{N(}#1, #2\textrm{) }\cap \textrm{ #3} \neq \emptyset}
     							   %  N(x, eps) intersect S \= emptyset
\newcommand{\nbhe}[3]{\textrm{N(}#1, #2\textrm{) }\cap \textrm{ #3} = \emptyset}
     							   %  N(x, eps) intersect S  = emptyset
\newcommand{\dnbho}[3]{\textrm{N*(}#1, #2\textrm{) }\cap \textrm{ #3} \neq \emptyset}
     							   %  N*(x, eps) intersect S \= emptyset
\newcommand{\dnbhe}[3]{\textrm{N*(}#1, #2\textrm{) }\cap \textrm{ #3} = \emptyset}
     							   %  N*(x, eps) intersect S = emptyset
     							   
\newcommand{\eqn}[1]{\[#1\]}
\newcommand{\splt}[1]{\begin{split}#1\end{split}}

\newcommand{\txt}[1]{\text{#1}} % Not new command, but remember \text for text in eqns
\newcommand{\tl}{\mt{\thicksim}}
\newcommand{\mn}[1]{\mt{\overline{#1}}}
\newcommand{\sg}{\mt{\sigma}}
\newcommand{\ssq}{\mt{\sigma^2}}	

\newcommand{\bh}[1]{\mathbf{\hat{\text{$#1$}}}}
\newcommand{\bth}{\mt{\bh{\beta}}}
\newcommand{\yh}{\mt{\bh{Y}}}

\newcommand{\exv}[1]{\txt{E[}#1\txt{]}}
\newcommand{\vrn}[1]{V[#1]}

\newcommand{\gv}{ \mt{|} }

\newcommand{\cov}[2]{\txt{Cov(#1, #2)}}

\newcommand{\img}[1]{
\begin{figure}[h]
  \includegraphics[width=0.5\linewidth]{#1}
\end{figure}
}
\newcommand\tab[1][1cm]{\hspace*{#1}}	
\newcommand{\sumin}[1]{\mt{\sum_{i = 1}^n #1}}		 
% ----------

\begin{document}
\begin{table}[ht]
\centering
\begin{tabular}{llllll}
  \hline
 & Q1 & Q2 & Q3 & Q4 & Q5 \\ 
  \hline
50 Points & 10 & 14 & 8 & 8 & 10 \\ 
   &  &  &  &  &  \\ 
   \hline
\end{tabular}
\end{table}
\bsc{Question 1}{
For multiple regression
\eqn{y \eql X\beta \ps \ep, \txt{ \ep\tl N(0, \ssq)}}
\eqn{\underset{n\times 1}{y}\tab\underset{n\times p}{X}\tab\underset{p\times 1}{\bta}\tab\underset{n\times 1}{\ep}}
Derive or show that

\balist
\item \bth \eql \uf{(X\pr X)}{-1}X\pr Y
	\eqn{y \eql X\beta \ps \ep}
	\eqn{\txt{Minimize: } S(\bta) = \sumin{\ep_i^2} = \ep\pr\ep}
	\eqn{
		\splt{
			S(\bta) & = (y - X\bta)\pr(y - X\bta) \\
			  & = y\pr y - \bta\pr X\pr y - y\pr X\bta + \bta\pr X\pr X\bta \\
			  & \txt{(since }\bta\pr X\pr y\txt{ is 1 x 1, }\bta\pr X\pr y = y\pr X\bta\txt{)} \\
			  & = y\pr y - 2\bta\pr X\pr y + \bta\pr X\pr X\bta
			}
		}
	So,
	\eqn{\frac{\partial S}{\partial \bta}\Big{|}_{\bth} = -2X\pr y + 2X\pr X\bth}
	\eqn{-2X\pr y + 2X\pr X\bth = 0}
	\eqn{2X\pr X\bth = 2X\pr y}
	\eqn{X\pr X\bth = X\pr y}
	\eqn{\bth = (X\pr X)^{-1}X\pr y}
\item \exv{\bth} \eql \bta
	
	\eqn{
		\splt{
			\exv{\bth} & = \exv{(X\pr X)^{-1}X\pr y} \\
			& = (X\pr X)^{-1}X\pr \exv{y} \\
			& = (X\pr X)^{-1}X\pr (X\bta + 0) \\
			& = (X\pr X)^{-1}X\pr X\bta \\
			& = \bta
		}
	}
	
\item \vrn{\bth} \eql \ssq\uf{(X\pr X)}{-1}
	
	\eqn{
		\splt{
			\vrn{\bth} & = \vrn{(X\pr X)^{-1}X\pr y} \\
			& = (X\pr X)^{-1}X\pr \times \vrn{y} \times ((X\pr X)^{-1}X\pr)\pr \\
			& = (X\pr X)^{-1}X\pr \times \vrn{y} \times X((X\pr X)^{-1})\pr \\
			& = (X\pr X)^{-1}X\pr \times \vrn{y} \times X((X\pr X)\pr)^{-1} \\
			& = (X\pr X)^{-1}X\pr \times \vrn{y} \times X(X\pr X)^{-1} \\
			& = (X\pr X)^{-1}X\pr \times  X(X\pr X)^{-1} \times \vrn{y} \\
			& = (X\pr X)^{-1}X\pr X(X\pr X)^{-1} \times \vrn{y} \\
			& = (X\pr X)^{-1} \vrn{y} \\
			& = \ssq\uf{(X\pr X)}{-1}
		}
	}
	
\item \exv{\yh} \eql X\bta
	
	\eqn{
		\splt{
			\exv{\yh} & = \exv{\uw{\bth}{0} + \uw{\bth}{1}\uw{X}{1} + \uw{\bth}{2}\uw{X}{2} ...} \\
			& = \exv{X\bth} \\
			& = X \times \exv{\bth} \\
			& = X\bta
		}
	}
\item \vrn{\yh} \eql \ssq H, where H is the hat matrix and H \eql X\uf{(X\pr X)}{-1}X\pr

	\eqn{
		\splt{
			\vrn{\yh} & = \vrn{\uw{\bth}{0} + \uw{\bth}{1}\uw{X}{1} + \uw{\bth}{2}\uw{X}{2} ...} \\
			& = \vrn{X\bth} \\
			& = X \vrn{\bth}X\pr \\
			& = X\ssq\uf{(X\pr X)}{-1}X\pr	\\
			& = \ssq X\uf{(X\pr X)}{-1}X\pr \\
			& = \ssq H
		}
	}

\elist

}

\bsc{Question 2 (problems 3.1 and 3.3 on page 121)}{
\balist
\item Fit a multiple linear regression model relating the number of games won to the team's passing yardage (\uw{x}{2}), the percentage of rushing plays (\uw{x}{7}), and the opponents' yards rushing (\uw{x}{8}).

	\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & -1.8084 & 7.9009 & -0.23 & 0.8209 \\ 
  x\$x2 & 0.0036 & 0.0007 & 5.18 & 0.0000 \\ 
  x\$x7 & 0.1940 & 0.0882 & 2.20 & 0.0378 \\ 
  x\$x8 & -0.0048 & 0.0013 & -3.77 & 0.0009 \\ 
   \hline
\end{tabular}
\end{table}
\item Construct the analysis-of-variance table and test for significance of regression.
	
	\begin{table}[ht]
\centering
\begin{tabular}{lrrrrr}
  \hline
 & Df & Sum Sq & Mean Sq & F value & Pr($>$F) \\ 
  \hline
x\$x2 & 1 & 76.19 & 76.19 & 26.17 & 0.0000 \\ 
  x\$x7 & 1 & 139.50 & 139.50 & 47.92 & 0.0000 \\ 
  x\$x8 & 1 & 41.40 & 41.40 & 14.22 & 0.0009 \\ 
  Residuals & 24 & 69.87 & 2.91 &  &  \\ 
   \hline
\end{tabular}
\end{table}

\ 

\

\

To test for significance of regression, we establish \uw{H}{0} and \uw{H}{a}:

\uw{H}{0}: \uw{\bta}{2} \eql \uw{\bta}{7} \eql \uw{\bta}{8} \eql 0 

\uw{H}{a}: \uw{\bta}{j} $\neq$ 0 for at least one of j \eql 2, 7, 8

We reject \uw{H}{0} if \uw{F}{0, j} \gr \uw{F}{0.05 \eql \afa, \tab 9,\tab 18 \eql (28 - 9 - 1)} for any \uw{F}{0, j} \textbf{*** 28\ms9\ms1 or 28\ms3\ms1?}

\uw{F}{0, 2} \eql 26.17 \gr 2.4563

\uw{F}{0, 7} \eql 47.92 \gr 2.4563

\uw{F}{0, 8} \eql 14.22 \gr 2.4563

So, reject \uw{H}{0}. There is evidence to conclude that there is a linear relationship for y \tl \uw{x}{2}, y \tl \uw{x}{7}, and y \tl \uw{x}{8}

\item Calculate t statistics for testing the hypotheses \uw{H}{0}: \uw{\bta}{2} \eql 0, \uw{H}{0}: \uw{\bta}{7} \eql 0, \uw{H}{0}: \uw{\bta}{8} \eql 0. What conclusions can you draw about the roles the variables \uw{x}{2}, \uw{x}{7}, and \uw{x}{8} play in the model?

	\bpth{1} R:
	\bilist
	\item \uw{H}{0}: \uw{\bta}{2} \eql 0
		
		\uw{\bta}{2} \eql 0.003598, t \eql 5.177, \uw{t}{\frc{0.05}{2}, 24} \eql 2.064 \lra \av{5.117} \gr 2.064 \rar Reject \uw{H}{0}
	\item \uw{H}{0}: \uw{\bta}{7} \eql 0
		
		\uw{\bta}{7} \eql 0.193960, t \eql 2.198, \uw{t}{\frc{0.05}{2}, 24} \eql 2.064 \lra \av{2.198} \gr 2.064 \rar Reject \uw{H}{0}
	\item \uw{H}{0}: \uw{\bta}{8} \eql 0
		
		\uw{\bta}{8} \eql -0.004816, t \eql -3.771, \uw{t}{\frc{0.05}{2}, 24} \eql 2.064 \lra \av{-3.771} \gr 2.064 \rar Reject \uw{H}{0}
	\elist
	\bpth{2} Manual:
	\bilist
	\item \uw{H}{0}: \uw{\bta}{2} \eql 0
		
		\uw{\bta}{2} \eql 0.003598, \uw{t}{\frc{0.05}{2}, 24} \eql 2.064
		\eqn{
			\splt{
				t & = \frac{\uw{\bth}{2} - 0}{se(\uw{\bth}{2})} \\
				 & = \frac{0.003598}{0.000695} \\
				 & = 5.177
			}
		}
		
		\av{5.117} \gr 2.064 \rar Reject \uw{H}{0}
	\item \uw{H}{0}: \uw{\bta}{7} \eql 0
		
		\uw{\bta}{7} \eql 0.193960, \uw{t}{\frc{0.05}{2}, 24} \eql 2.064
		\eqn{
			\splt{
				t & = \frac{\uw{\bth}{7} - 0}{se(\uw{\bth}{7})} \\
				 & = \frac{0.193960}{0.088233} \\
				 & = 2.198
			}
		}
		
		\av{2.198} \gr 2.064 \rar Reject \uw{H}{0}
	\item \uw{H}{0}: \uw{\bta}{8} \eql 0
		
		\uw{\bta}{8} \eql -0.004816, \uw{t}{\frc{0.05}{2}, 24} \eql 2.064
		\eqn{
			\splt{
				t & = \frac{\uw{\bth}{8} - 0}{se(\uw{\bth}{8})} \\
				 & = \frac{-0.004816}{0.001277} \\
				 & = \ms3.771
			}
		}
		
		\av{ \ms3.771 } \gr 2.064 \rar Reject \uw{H}{0}
	\elist
\item Calculate \uf{R}{2} and \uw{\uf{R}{2}}{adj} for this model.
	
	\bpth{1} R:
	
	\uf{R}{2} \lra summary(model)\$r.squared yields \textbf{0.7863069}
	
	\uw{\uf{R}{2}}{adj} \lra  summary(model)\$adj.r.squared yields \textbf{0.7595953}
	
	\bpth{2} Manual:
	
	Knowing: \uw{SS}{T} \eql \uw{SS}{R} \ps \uw{SS}{res}
	
	From anova(model) in R:
	
	\uw{SS}{T} \eql (76.193 \ps 139.501 \ps 41.400) (\uw{SS}{R}) \ps 69.870 (\uw{SS}{res}) \eql 326.964
	
	\eqn{
		\splt{
			\uf{R}{2} & = 1 - \frac{SS_{res}}{SS_{T}} \\
			& = 1 - \frac{69.870}{326.964} \\
			& = 0.7863067
		}
	}
	
	\eqn{
		\splt{
			\uf{R}{2}_{\txt{adj}} & = \frac{1 - \frac{SS_{\txt{res}}}{(n - p)}}{\frac{SS_\txt{T}}{(n - 1)}} \\
			& = 1 - \frac{SS_{res}(n - 1)}{SS_T(n - k - 1)} \\
			& = 1 - \frac{69.870(27)}{326.964(24)} \\
			& = 0.7595951
		}
	}
	
\item Using the partial F test, determine the contribution of \uw{x}{7} to the model. How is this partial F statistic related to the t test for \uw{\bta}{7} calculated in part c above?
	
	\textbf{Knowing:} 
	
	The partial F-test is the most common method of testing for a nested normal linear regression model. "Nested" model is just a fancy way of saying a reduced model in terms of variables included.
	
	If \uw{F}{0} \gr \uw{F}{\afa, r, n \ms p}, we reject \uw{H}{0}, concluding that at least one of the parameters in \uw{\bta}{2} is not zero, and consequently at least one of the regressors \uw{x}{k \ms r \ps 1}, \uw{x}{k \ms r \ps 2}, . . . , \uw{x}{k} in \uw{X}{2} contribute significantly to the regression model. Some authors call the test in (3.35) a partial F test because it measures the contribution of the regressors in \uw{X}{2} given that the other regressors in \uw{X}{1} are in the model.
	
	Partial F-Test:
	
	\uw{H}{0}: \uw{\bta}{2} \eql 0
	
	\eqn{F_0 = \frac{SS_R(\bta_1|\bta_2)}{r\times MS_{res}}}
	
	where \uw{\bta}{1} \eql \bta \ms \bk{\uw{\bta}{7}}, \uw{\bta}{2} \eql \uw{\bta}{7}
	
	\eqn{SS_R(\bta_2|\bta_1) = SS_R(\bta) - SS_R(\bta_1)}
	\eqn{SS_R(\bta_2|\bta_1) = (76.193 + 139.501 + 41.400) - (76.193 + 41.400)}
	\eqn{= 139.501}
	
	r \eql 1
	
	\uw{MS}{res} \eql 2.911
	
	\eqn{F_0 = \frac{139.501}{1 \times 2.911}}
	\eqn{= 47.92202}
	
	\uw{F}{\afa, r, n \ms p} \eql \uw{F}{0.05, 1, (28 \ms (3 \ps 1) \eql 24} \eql 4.2597
	
	Reject \uw{H}{0} if \uw{F}{0} \gr \uw{F}{0.05, 1, 24}
	
	47.92202 \gr 4.2597 \lra reject \uw{H}{0}
	
	anova(lm(y \tl x7))\$F yields 11.00524
	
	qf(0.025, df1 = 1, df2 = 24, lower.tail = F) yields 5.713369
	
\item Find a 95\% CI on \uw{\bta}{7}. (This is part a of problem 3.3, and the following one is part b of problem 3.3.)
	
	\bpth{1} R:
	
	\bpth{2} Manual:
	
	A CI for \uw{\bta}{j} is \uw{\bth}{j} (\ps or \ms) \uw{t}{\frc{\afa}{2}, n \ms p}SE(\uw{\bth}{j})
	
	\uw{\bth}{7} \eql 0.193960
	
	\uw{t}{\frc{\afa}{2}, n \ms p} \eql \uw{t}{0.025, 28 \ms 4 \eql 24} \eql 2.064
	
	SE(\uw{\bth}{j} \eql 0.088233
	
	(0.193960 \ms (2.064$\times$0.088233), 0.193960 \ps (2.064$\times$0.088233)
	
\item Find a 95\% CI on the mean number of games won by a team when \uw{x}{2} \eql 2300, \uw{x}{7} \eql 56.0, and \uw{x}{8} \eql 2100.
	
	
	\bpth{1} R:
	
	prediction(
	
	\bpth{2} Manual:
	
\elist


Note: For c, d, f, and g, please show two versions of your results: (1) obtained using R code and (2) based on your manual calculation (please show detailed step for your manual calculation. You can use the partial output from the lm or ANOVA, e.g., the \uw{SS}{reg}, \uw{SS}{res}, the estimated value of \bta and its variance or standard deviation). If you can show how to get the t-statistics (or CI, R-square) based on part of the output obtained from R, that will be fine.

}

\bsc{Question 3 (Exercise 3.4 on page 122)}{
Reconsider the National Football League data from Problem 3.1. Fit a model to this data using only \uw{x}{7} and \uw{x}{8} as the regressors.
\balist
\item Test for significance of the regression.
\item Calculate \uf{R}{2} and \uw{\uf{R}{2}}{adj}. How do these quantities compare to the values computed for the model in problem 3.1, which included an additional regressor (\uf{x}{2})?
\item Calculate a 95\% CI on \uw{\bta}{7}. Also, find a 95\% CI on the mean number of games won by a team when \uw{x}{7} \eql 56.0 and \uw{x}{8} \eql 2100. Compare the lengths of these CIs to the lengths of the corresponding CIs from problem 3.3 (that is, the above part f and g in question 2)
\item What conclusions can you draw from this problem about the consequences of omitting an important regressor from a model?
\elist

}

\bsc{Question 4 (exercise 4.2 on page 165)}{
Consider the multiple regression model fit to the National Football League (NFL) team performance data in problem 3.1.

can use qq norm for this one

\balist
\item Construct a normal probability plot of the residuals. Does there seem to be any problem with the normality assumption?
\item Construct and interpret a plot of the residuals versus the predicted response.
\item Construct plots of the residuals versus each of the regressor variables. Do these plots imply that the regressor is correctly specified?
\item Construct the partial regression plots for this model. Compare the plots with the plots of residuals versus regressors from part c above. Discuss the type of information provided by these plots.
\elist
}

\bsc{Question 5}{
Show that the hat matrix H \eql X\uf{(X\pr X)}{-1}X\pr and I \ms H (where I is the identity matrix) are symmetric and idempotent. That is, please show:
\balist
\item H\pr \eql H and HH \eql H (H\pr\txt{ }means the transpose of H, HH means H * H)
	\eqn{
		\splt{
			H & = X\uf{(X\pr X)}{-1}X\pr \\
			H\pr & = (X\uf{(X\pr X)}{-1}X\pr)\pr \\
			& = X((X\pr X)^{-1})\pr X\pr \\
			& = X((X\pr X)\pr)^{-1}X\pr \\
			& = X(X\pr X)^{-1}X\pr \\
			& = H
		}
	}
	
	\eqn{
		\splt{
			H & = X\uf{(X\pr X)}{-1}X\pr \\
			HH & = (X\uf{(X\pr X)}{-1}X\pr)(X\uf{(X\pr X)}{-1}X\pr) \\
			HH & = X\uf{(X\pr X)}{-1}X\pr  X\uf{(X\pr X)}{-1}X\pr \\
			& = X\uf{(X\pr X)}{-1}X\pr \\
			& = H
		}
	}
\item (I \ms H)\pr \eql I \ms H and (I \ms H)(I \ms H) \eql I \ms H
	
	\eqn{
		\splt{
			(I - H)\pr & = (I - X\uf{(X\pr X)}{-1}X\pr)\pr \\
			& = I\pr - (X\uf{(X\pr X)}{-1}X\pr)\pr)\pr \\
			& = I - (X\uf{(X\pr X)}{-1}X\pr)\pr)\pr \\
			& = I - X\uf{(X\pr X)}{-1}X\pr \\
			& = I - H
		}
	}
	
	\eqn{
		\splt{
			(I - H)(I - H) & = (I - X\uf{(X\pr X)}{-1}X\pr)(I - X\uf{(X\pr X)}{-1}X\pr) \\
			& = I - 2X\uf{(X\pr X)}{-1}X\pr + (X\uf{(X\pr X)}{-1}X\pr)(X\uf{(X\pr X)}{-1}X\pr) \\
			& = I - 2X\uf{(X\pr X)}{-1}X\pr + X\uf{(X\pr X)}{-1}X\pr \txt{ by (a)} \\
			& = I - X\uf{(X\pr X)}{-1}X\pr \\
			& = I - H
		}
	}
\elist

Hint: A\eql X\pr X is a symmetric matrix, and for a symmetric matrix, \uf{(A\pr)}{-1} \eql (\uf{A}{-1})\pr. You can use this property directly in your proof of \bpth{a} and \bpth{b}. If you are interested in the proof of this property, you may check the following web page:

https://math.stackexchange.com/questions/325082/is-the-inverse-of-a-symmetric-matrix-also-symmetric

}
\end{document}