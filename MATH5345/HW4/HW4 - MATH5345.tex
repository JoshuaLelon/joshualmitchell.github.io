% Thank you Josh Davis for this template!
% https://github.com/jdavis/latex-homework-template/blob/master/homework.tex

\documentclass{article}

\newcommand{\hmwkTitle}{HW\ \#4}

% \input{ShortcutsStatistics}

% ----------

% Packages

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}
\usepackage{enumitem}
\usepackage{chngcntr}

% Libraries

\usetikzlibrary{automata, positioning, arrows}

%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{}
\rhead{\hmwkClass\ (\hmwkClassInstructor): \hmwkTitle}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}
\setcounter{secnumdepth}{0}

\newcommand{\hmwkClass}{MATH 5345 / Regression Analysis}        % Class
\newcommand{\hmwkClassInstructor}{Dr. Sun}           % Instructor
\newcommand{\hmwkAuthorName}{\textbf{Joshua Mitchell}} % Author

%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small\vspace{0.1in}\large{\textit{\hmwkClassInstructor}}
    \vspace{3in}
}

\author{\hmwkAuthorName}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

%
% Various Helper Commands
%

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}


% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Formatting commands:

\newcommand{\mt}[1]{\ensuremath{#1}}
\newcommand{\nm}[1]{\textrm{#1}}

\newcommand\bsc[2][\DefaultOpt]{%
  \def\DefaultOpt{#2}%
  \section[#1]{#2}%
}
\newcommand\ssc[2][\DefaultOpt]{%
  \def\DefaultOpt{#2}%
  \subsection[#1]{#2}%
}
\newcommand{\bgpf}{\begin{proof} $ $\newline}

\newcommand{\bgeq}{\begin{equation*}}
\newcommand{\eeq}{\end{equation*}}	

\newcommand{\balist}{\begin{enumerate}[label=\alph*.]}
\newcommand{\elist}{\end{enumerate}}

\newcommand{\bilist}{\begin{enumerate}[label=\roman*)]}	

\newcommand{\bgsp}{\begin{split}}
% \newcommand{\esp}{\end{split}} % doesn't work for some reason.

\newcommand\prs[1]{~~~\textbf{(#1)}}

\newcommand{\lt}[1]{\textbf{Let: } #1}
     							   %  if you're setting it to be true
\newcommand{\supp}[1]{\textbf{Suppose: } #1}
     							   %  Suppose (if it'll end up false)
\newcommand{\wts}[1]{\textbf{Want to show: } #1}
     							   %  Want to show
\newcommand{\as}[1]{\textbf{Assume: } #1}
     							   %  if you think it follows from truth
\newcommand{\bpth}[1]{\textbf{(#1)}}

\newcommand{\step}[2]{\begin{equation}\tag{#2}#1\end{equation}}
\newcommand{\epf}{\end{proof}}

\newcommand{\dbs}[3]{\mt{#1_{#2_#3}}}

\newcommand{\sidenote}[1]{-----------------------------------------------------------------Side Note----------------------------------------------------------------
#1 \

---------------------------------------------------------------------------------------------------------------------------------------------}

% Analysis / Logical commands:

\newcommand{\br}{\mt{\mathbb{R}} }       % |R
\newcommand{\bq}{\mt{\mathbb{Q}} }       % |Q
\newcommand{\bn}{\mt{\mathbb{N}} }       % |N
\newcommand{\bc}{\mt{\mathbb{C}} }       % |C
\newcommand{\bz}{\mt{\mathbb{Z}} }       % |Z

\newcommand{\ep}{\mt{\epsilon} }         % epsilon
\newcommand{\fa}{\mt{\forall} }          % for all
\newcommand{\afa}{\mt{\alpha} }
\newcommand{\bta}{\mt{\beta} }
\newcommand{\mem}{\mt{\in} }
\newcommand{\exs}{\mt{\exists} }

\newcommand{\es}{\mt{\emptyset} }        % empty set
\newcommand{\sbs}{\mt{\subset} }         % subset of
\newcommand{\fs}[2]{\{\uw{#1}{1}, \uw{#1}{2}, ... \uw{#1}{#2}\}}

\newcommand{\lra}{ \mt{\longrightarrow} } % implies ----->
\newcommand{\rar}{ \mt{\Rightarrow} }     % implies -->

\newcommand{\lla}{ \mt{\longleftarrow} }  % implies <-----
\newcommand{\lar}{ \mt{\Leftarrow} }      % implies <--

\newcommand{\av}[1]{\mt{|}#1\mt{|}}  % absolute value

\newcommand{\prn}[1]{(#1)}
\newcommand{\bk}[1]{\{#1\}}

\newcommand{\ps}{\mt{\operatorname{+}} }
\newcommand{\ms}{\mt{\operatorname{-}} }

\newcommand{\ls}{\mt{\operatorname{<}} }
\newcommand{\gr}{\mt{\operatorname{>}} }

\newcommand{\lse}{\mt{\operatorname{\leq}} }
\newcommand{\gre}{\mt{\operatorname{\geq}} }

\newcommand{\eql}{ \mt{\operatorname{=}} }

\newcommand{\pr}{\mt{^\prime}} 		   % prime (i.e. R')
\newcommand{\uw}[2]{#1\mt{_{#2}}}
\newcommand{\uf}[2]{#1\mt{^{#2}}}
\newcommand{\frc}[2]{\mt{\frac{#1}{#2}}}
\newcommand{\lmti}[1]{\mt{\displaystyle{\lim_{#1 \to \infty}}}}
\newcommand{\limt}[2]{\mt{\displaystyle{\lim_{#1 \to #2}}}}

\newcommand{\bnm}[2]{\mt{#1\setminus{#2}}}
\newcommand{\bnt}[2]{\mt{\textrm{#1}\setminus{\textrm{#2}}}}
\newcommand{\bi}{\bnm{\mathbb{R}}{\mathbb{Q}}}

\newcommand{\urng}[2]{\mt{\bigcup_{#1}^{#2}}}
\newcommand{\nrng}[2]{\mt{\bigcap_{#1}^{#2}}}
\newcommand{\nck}[2]{\mt{{#1 \choose #2}}}

\newcommand{\nbho}[3]{\textrm{N(}#1, #2\textrm{) }\cap \textrm{ #3} \neq \emptyset}
     							   %  N(x, eps) intersect S \= emptyset
\newcommand{\nbhe}[3]{\textrm{N(}#1, #2\textrm{) }\cap \textrm{ #3} = \emptyset}
     							   %  N(x, eps) intersect S  = emptyset
\newcommand{\dnbho}[3]{\textrm{N*(}#1, #2\textrm{) }\cap \textrm{ #3} \neq \emptyset}
     							   %  N*(x, eps) intersect S \= emptyset
\newcommand{\dnbhe}[3]{\textrm{N*(}#1, #2\textrm{) }\cap \textrm{ #3} = \emptyset}
     							   %  N*(x, eps) intersect S = emptyset
     							   
\newcommand{\eqn}[1]{\[#1\]}
\newcommand{\splt}[1]{\begin{split}#1\end{split}}

\newcommand{\txt}[1]{\text{#1}} % Not new command, but remember \text for text in eqns
\newcommand{\tl}{\mt{\thicksim}}
\newcommand{\mn}[1]{\mt{\overline{#1}}}
\newcommand{\sg}{\mt{\sigma}}
\newcommand{\ssq}{\mt{\sigma^2}}	

\newcommand{\bh}[1]{\mathbf{\hat{\text{$#1$}}}}
\newcommand{\bth}{\mt{\bh{\beta}}}
\newcommand{\yh}{\mt{\bh{Y}}}

\newcommand{\exv}[1]{\txt{E[}#1\txt{]}}
\newcommand{\vrn}[1]{V[#1]}

\newcommand{\gv}{ \mt{|} }

\newcommand{\cov}[2]{\txt{Cov(#1, #2)}}

\newcommand{\img}[1]{
\begin{figure}[h]
  \includegraphics[width=0.5\linewidth]{#1}
\end{figure}
}
\newcommand{\simg}[1]{
  \includegraphics[width=0.35\linewidth]{#1}
}

\newcommand\tab[1][1cm]{\hspace*{#1}}	
\newcommand{\sumin}[1]{\mt{\sum_{i = 1}^n #1}}		 
% ----------

\begin{document}
\begin{table}[ht]
\centering
\begin{tabular}{llllll}
  \hline
 & Q1 & Q2 & Q3 & Q4 & Q5 \\ 
  \hline
50 Points & 10 & 14 & 8 & 8 & 10 \\ 
   &  &  &  &  &  \\ 
   \hline
\end{tabular}
\end{table}
\bsc{Question 1}{
For multiple regression
\eqn{y \eql X\beta \ps \ep, \txt{ \ep\tl N(0, \ssq)}}
\eqn{\underset{n\times 1}{y}\tab\underset{n\times p}{X}\tab\underset{p\times 1}{\bta}\tab\underset{n\times 1}{\ep}}
Derive or show that

\balist
\item \bth \eql \uf{(X\pr X)}{-1}X\pr Y
	\eqn{y \eql X\beta \ps \ep}
	\eqn{\txt{Minimize: } S(\bta) = \sumin{\ep_i^2} = \ep\pr\ep}
	\eqn{
		\splt{
			S(\bta) & = (y - X\bta)\pr(y - X\bta) \\
			  & = y\pr y - \bta\pr X\pr y - y\pr X\bta + \bta\pr X\pr X\bta \\
			  & \txt{(since }\bta\pr X\pr y\txt{ is 1 x 1, }\bta\pr X\pr y = y\pr X\bta\txt{)} \\
			  & = y\pr y - 2\bta\pr X\pr y + \bta\pr X\pr X\bta
			}
		}
	So,
	\eqn{\frac{\partial S}{\partial \bta}\Big{|}_{\bth} = -2X\pr y + 2X\pr X\bth}
	\eqn{-2X\pr y + 2X\pr X\bth = 0}
	\eqn{2X\pr X\bth = 2X\pr y}
	\eqn{X\pr X\bth = X\pr y}
	\eqn{\bth = (X\pr X)^{-1}X\pr y}
\item \exv{\bth} \eql \bta
	
	\eqn{
		\splt{
			\exv{\bth} & = \exv{(X\pr X)^{-1}X\pr y} \\
			& = (X\pr X)^{-1}X\pr \exv{y} \\
			& = (X\pr X)^{-1}X\pr (X\bta + 0) \\
			& = (X\pr X)^{-1}X\pr X\bta \\
			& = \bta
		}
	}
	
\newpage
	
\item \vrn{\bth} \eql \ssq\uf{(X\pr X)}{-1}
	
	\eqn{
		\splt{
			\vrn{\bth} & = \vrn{(X\pr X)^{-1}X\pr y} \\
			& = (X\pr X)^{-1}X\pr \times \vrn{y} \times ((X\pr X)^{-1}X\pr)\pr \\
			& = (X\pr X)^{-1}X\pr \times \vrn{y} \times X((X\pr X)^{-1})\pr \\
			& = (X\pr X)^{-1}X\pr \times \vrn{y} \times X((X\pr X)\pr)^{-1} \\
			& = (X\pr X)^{-1}X\pr \times \vrn{y} \times X(X\pr X)^{-1} \\
			& = (X\pr X)^{-1}X\pr \times  X(X\pr X)^{-1} \times \vrn{y} \\
			& = (X\pr X)^{-1}X\pr X(X\pr X)^{-1} \times \vrn{y} \\
			& = (X\pr X)^{-1} \vrn{y} \\
			& = \ssq\uf{(X\pr X)}{-1}
		}
	}
	
\item \exv{\yh} \eql X\bta
	
	\eqn{
		\splt{
			\exv{\yh} & = \exv{\uw{\bth}{0} + \uw{\bth}{1}\uw{X}{1} + \uw{\bth}{2}\uw{X}{2} ...} \\
			& = \exv{X\bth} \\
			& = X \times \exv{\bth} \\
			& = X\bta
		}
	}
\item \vrn{\yh} \eql \ssq H, where H is the hat matrix and H \eql X\uf{(X\pr X)}{-1}X\pr

	\eqn{
		\splt{
			\vrn{\yh} & = \vrn{\uw{\bth}{0} + \uw{\bth}{1}\uw{X}{1} + \uw{\bth}{2}\uw{X}{2} ...} \\
			& = \vrn{X\bth} \\
			& = X \vrn{\bth}X\pr \\
			& = X\ssq\uf{(X\pr X)}{-1}X\pr	\\
			& = \ssq X\uf{(X\pr X)}{-1}X\pr \\
			& = \ssq H
		}
	}

\elist

}

\bsc{Question 2 (problems 3.1 and 3.3 on page 121)}{
\balist
\item Fit a multiple linear regression model relating the number of games won to the team's passing yardage (\uw{x}{2}), the percentage of rushing plays (\uw{x}{7}), and the opponents' yards rushing (\uw{x}{8}).

	\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & -1.8084 & 7.9009 & -0.23 & 0.8209 \\ 
  x\$x2 & 0.0036 & 0.0007 & 5.18 & 0.0000 \\ 
  x\$x7 & 0.1940 & 0.0882 & 2.20 & 0.0378 \\ 
  x\$x8 & -0.0048 & 0.0013 & -3.77 & 0.0009 \\ 
   \hline
\end{tabular}
\end{table}

\newpage 

\item Construct the analysis-of-variance table and test for significance of regression.
	
	\begin{table}[ht]
\centering
\begin{tabular}{lrrrrr}
  \hline
 & Df & Sum Sq & Mean Sq & F value & Pr($>$F) \\ 
  \hline
x\$x2 & 1 & 76.19 & 76.19 & 26.17 & 0.0000 \\ 
  x\$x7 & 1 & 139.50 & 139.50 & 47.92 & 0.0000 \\ 
  x\$x8 & 1 & 41.40 & 41.40 & 14.22 & 0.0009 \\ 
  Residuals & 24 & 69.87 & 2.91 &  &  \\ 
   \hline
\end{tabular}
\end{table}

To test for significance of regression, we establish \uw{H}{0} and \uw{H}{a}:

\uw{H}{0}: \uw{\bta}{2} \eql \uw{\bta}{7} \eql \uw{\bta}{8} \eql 0 

\uw{H}{a}: \uw{\bta}{j} $\neq$ 0 for at least one of j \eql 2, 7, 8

We reject \uw{H}{0} if \uw{F}{0, j} \gr F \eql 29.44 (from R)

\uw{F}{0, 2} \eql 26.17 \gr 29.44 (uh oh)

\uw{F}{0, 7} \eql 47.92 \gr 29.44 

\uw{F}{0, 8} \eql 14.22 \gr 29.44 (uh oh)

So, reject \uw{H}{0}. There is evidence to conclude that there is a linear relationship for y \tl \uw{x}{2}, y \tl \uw{x}{7}, and y \tl \uw{x}{8}

\item Calculate t statistics for testing the hypotheses \uw{H}{0}: \uw{\bta}{2} \eql 0, \uw{H}{0}: \uw{\bta}{7} \eql 0, \uw{H}{0}: \uw{\bta}{8} \eql 0. What conclusions can you draw about the roles the variables \uw{x}{2}, \uw{x}{7}, and \uw{x}{8} play in the model?

	\bpth{1} R:
	\bilist
	\item \uw{H}{0}: \uw{\bta}{2} \eql 0
		
		\uw{\bta}{2} \eql 0.003598, t \eql 5.177, \uw{t}{\frc{0.05}{2}, 24} \eql 2.064 \lra \av{5.117} \gr 2.064 \rar Reject \uw{H}{0}
	\item \uw{H}{0}: \uw{\bta}{7} \eql 0
		
		\uw{\bta}{7} \eql 0.193960, t \eql 2.198, \uw{t}{\frc{0.05}{2}, 24} \eql 2.064 \lra \av{2.198} \gr 2.064 \rar Reject \uw{H}{0}
	\item \uw{H}{0}: \uw{\bta}{8} \eql 0
		
		\uw{\bta}{8} \eql -0.004816, t \eql -3.771, \uw{t}{\frc{0.05}{2}, 24} \eql 2.064 \lra \av{-3.771} \gr 2.064 \rar Reject \uw{H}{0}
	\elist
	\bpth{2} Manual:
	\bilist
	\item \uw{H}{0}: \uw{\bta}{2} \eql 0
		
		\uw{\bta}{2} \eql 0.003598, \uw{t}{\frc{0.05}{2}, 24} \eql 2.064
		\eqn{
			\splt{
				t & = \frac{\uw{\bth}{2} - 0}{se(\uw{\bth}{2})} \\
				 & = \frac{0.003598}{0.000695} \\
				 & = 5.177
			}
		}
		
		\av{5.117} \gr 2.064 \rar Reject \uw{H}{0}
\newpage
	\item \uw{H}{0}: \uw{\bta}{7} \eql 0
		
		\uw{\bta}{7} \eql 0.193960, \uw{t}{\frc{0.05}{2}, 24} \eql 2.064
		\eqn{
			\splt{
				t & = \frac{\uw{\bth}{7} - 0}{se(\uw{\bth}{7})} \\
				 & = \frac{0.193960}{0.088233} \\
				 & = 2.198
			}
		}
		
		\av{2.198} \gr 2.064 \rar Reject \uw{H}{0}
	\item \uw{H}{0}: \uw{\bta}{8} \eql 0
		
		\uw{\bta}{8} \eql -0.004816, \uw{t}{\frc{0.05}{2}, 24} \eql 2.064
		\eqn{
			\splt{
				t & = \frac{\uw{\bth}{8} - 0}{se(\uw{\bth}{8})} \\
				 & = \frac{-0.004816}{0.001277} \\
				 & = \ms3.771
			}
		}
		
		\av{ \ms3.771 } \gr 2.064 \rar Reject \uw{H}{0}
	\elist
\item Calculate \uf{R}{2} and \uw{\uf{R}{2}}{adj} for this model.
	
	\bpth{1} R:
	
	\uf{R}{2} \lra summary(model)\$r.squared yields \textbf{0.7863069}
	
	\uw{\uf{R}{2}}{adj} \lra  summary(model)\$adj.r.squared yields \textbf{0.7595953}
	
	\bpth{2} Manual:
	
	Knowing: \uw{SS}{T} \eql \uw{SS}{R} \ps \uw{SS}{res}
	
	From anova(model) in R:
	
	\uw{SS}{T} \eql (76.193 \ps 139.501 \ps 41.400) (\uw{SS}{R}) \ps 69.870 (\uw{SS}{res}) \eql 326.964
	
	\eqn{
		\splt{
			\uf{R}{2} & = 1 - \frac{SS_{res}}{SS_{T}} \\
			& = 1 - \frac{69.870}{326.964} \\
			& = 0.7863067
		}
	}
	
	\eqn{
		\splt{
			\uf{R}{2}_{\txt{adj}} & = \frac{1 - \frac{SS_{\txt{res}}}{(n - p)}}{\frac{SS_\txt{T}}{(n - 1)}} \\
			& = 1 - \frac{SS_{res}(n - 1)}{SS_T(n - k - 1)} \\
			& = 1 - \frac{69.870(27)}{326.964(24)} \\
			& = 0.7595951
		}
	}
	
\item Using the partial F test, determine the contribution of \uw{x}{7} to the model. How is this partial F statistic related to the t test for \uw{\bta}{7} calculated in part c above?
	
	anova(lm(y \tl x7))\$F yields 11.00524
	
	qf(0.025, df1 = 1, df2 = 24, lower.tail = F) yields 5.713369
	
	11.00524 \gr 5.713369 \lra reject \uw{H}{0}
	
	The partial F statistic is related because you're essentially testing the full model (the regular model) vs the reduced model (without B$_7$) and seeing if the reduced model is actually a better model. This is equivalent to testing if B$_7$ \eql 0 with a t statistic: you're just evaluating the effect of B$_7$ on the model.
	
	\textbf{From the book:} 
	
	The partial F-test is the most common method of testing for a nested normal linear regression model. "Nested" model is just a fancy way of saying a reduced model in terms of variables included.
	
	If \uw{F}{0} \gr \uw{F}{\afa, r, n \ms p}, we reject \uw{H}{0}, concluding that at least one of the parameters in \uw{\bta}{2} is not zero, and consequently at least one of the regressors \uw{x}{k \ms r \ps 1}, \uw{x}{k \ms r \ps 2}, . . . , \uw{x}{k} in \uw{X}{2} contribute significantly to the regression model. Some authors call the test in (3.35) a partial F test because it measures the contribution of the regressors in \uw{X}{2} given that the other regressors in \uw{X}{1} are in the model.		
	
\item Find a 95\% CI on \uw{\bta}{7}. (This is part a of problem 3.3, and the following one is part b of problem 3.3.)
	
	\textbf{Question: Why does R say t \eql 2.198 with summary(model)? But 2.064 works for calculations?}
	
	\bpth{1} R:
	
	confint(model)
	
	x\$x7: (0.011855322, 0.376065098)
	
	\bpth{2} Manual:
	
	A CI for \uw{\bta}{j} is \uw{\bth}{j} (\ps or \ms) \uw{t}{\frc{\afa}{2}, n \ms p}SE(\uw{\bth}{j})
	
	\uw{\bth}{7} \eql 0.193960
	
	\uw{t}{\frc{\afa}{2}, n \ms p} \eql \uw{t}{0.025, 28 \ms 4 \eql 24} \eql 2.064
	
	SE(\uw{\bth}{j}) \eql 0.088233
	
	(0.193960 \ms (2.064$\times$0.088233), 0.193960 \ps (2.064$\times$0.088233)
	
	(0.011847088, 0.376072912)
	
\item Find a 95\% CI on the mean number of games won by a team when \uw{x}{2} \eql 2300, \uw{x}{7} \eql 56.0, and \uw{x}{8} \eql 2100.
	
	
	\bpth{1} R:
\begin{verbatim}
	test_X <- data.frame(x2=2300, x7=56, x8=2100)
	predict(model, test_X, interval="confidence")
	
	fit      lwr      upr
	7.216424 6.436203 7.996645
\end{verbatim}

\newpage

	\bpth{2} Manual:
	
	A CI for \exv{\uw{y}{0}$|$\uw{x}{0}} is \yh$_0$ (\ps or \ms) \uw{t}{\frc{\afa}{2}, n \ms p} $\sqrt{x_0\pr(x\pr x)^{-1}x_0\hat\ssq}$
	
	 \yh$_0$ \eql 7.216424 
	
	\uw{t}{\frc{\afa}{2}, n \ms p} \eql \uw{t}{0.025, 28 \ms 4 \eql 24} \eql 2.064
	
	\eqn{
		\splt{
			& \yh_0 \pm \uw{t}{\frc{\afa}{2}, n \ms p}\sqrt{x_0\pr(x\pr x)^{-1}x_0\hat\ssq} \\
			& 7.216424 \pm 2.064\sqrt{x_0\pr(x\pr x)^{-1}x_0(2.911)} \\
			& 7.216424 \pm 2.064\sqrt{0.04908781253(2.911)} \\
			& (6.436203, 7.996645)
		}
	}
\elist


Note: For c, d, f, and g, please show two versions of your results: (1) obtained using R code and (2) based on your manual calculation (please show detailed step for your manual calculation. You can use the partial output from the lm or ANOVA, e.g., the \uw{SS}{reg}, \uw{SS}{res}, the estimated value of \bta and its variance or standard deviation). If you can show how to get the t-statistics (or CI, R-square) based on part of the output obtained from R, that will be fine.

}

\bsc{Question 3 (Exercise 3.4 on page 122)}{
Reconsider the National Football League data from Problem 3.1. Fit a model to this data using only \uw{x}{7} and \uw{x}{8} as the regressors.
\balist
\item Test for significance of the regression (using only \uw{x}{7} and \uw{x}{8}).

	To test for significance of regression, we establish \uw{H}{0} and \uw{H}{a}:

	\uw{H}{0}: \uw{\bta}{7} \eql \uw{\bta}{8} \eql 0 

	\uw{H}{a}: \uw{\bta}{j} $\neq$ 0 for at least one of j \eql 7, 8

	We reject \uw{H}{0} if \uw{F}{0, j} \gr 15.13
	
	\uw{F}{0, 7} \eql 16.437 \gr 15.13

	\uw{F}{0, 8} \eql 13.832 \gr 15.13 (uh oh)

	So, reject \uw{H}{0}. There is evidence to conclude that there is a linear relationship for y \tl \uw{x}{7}, and y \tl \uw{x}{8}
	
	
\item Calculate \uf{R}{2} and \uw{\uf{R}{2}}{adj}. How do these quantities compare to the values computed for the model in problem 3.1, which included an additional regressor (\uf{x}{2})?
	
	summary(model)\$r.squared \lra 0.5476628
	
	summary(model)\$adj.r.squared \lra 0.5114759
	
	They're lower than the other values, which means that the model fits the data less effectively (i.e. SS$_{Residual}$ is higher)
	
\item Calculate a 95\% CI on \uw{\bta}{7}.

	confint(model)
	
	x7: (-0.19716429,  0.293906022)
	
\newpage
	
\item Also, find a 95\% CI on the mean number of games won by a team when \uw{x}{7} \eql 56.0 and \uw{x}{8} \eql 2100. Compare the lengths of these CIs to the lengths of the corresponding CIs from problem 3.3 (that is, the above part f and g in question 2)
	\begin{verbatim}
	test_X <- data.frame(x7=56, x8=2100)
	predict(model, test_X, interval="confidence")
    
    fit      lwr      upr
	6.926243 5.828643 8.023842
	\end{verbatim}
	
	The CI is definitely wider (since it's a less accurate model it has to be wider to be 95\% sure).
	
\item What conclusions can you draw from this problem about the consequences of omitting an important regressor from a model?
	
	If you leave out a regressor, you can make your model a lot less accurate.
\elist
}

\newpage

\bsc{Question 4 (exercise 4.2 on page 165)}{
Consider the multiple regression model fit to the National Football League (NFL) team performance data in problem 3.1.

can use qq norm for this one

\balist
\item Construct a normal probability plot of the residuals. Does there seem to be any problem with the normality assumption?
	
	model.resid = resid(model)
	
	qqnorm(model.resid, main = "Games Won vs Passing Yards / Rushing",
       xlab = "Passing Yards / \% Rushing / Opponents Rushing", ylab = "Games Won")
	
	qqline(model.resid)
	
	\img{q4a.png}
	
	I don't think so. Since the model's data follows an imagined normal distribution line fairly closely, it seems reasonable to assume normality.

\newpage

\item Construct and interpret a plot of the residuals versus the predicted response.
	
	plot(model\$fitted.values, model.resid)
	
	\img{q4b.png}
	
	It looks like static, indicating that there is no relationship between the residuals and predicted response, supporting the assumption that the errors are independent.

\item Construct plots of the residuals versus each of the regressor variables. Do these plots imply that the regressor is correctly specified?
	
	plot(x\$x2, model.resid)
	
	plot(x\$x7, model.resid)
	
	plot(x\$x8, model.resid)
	
	\simg{q4c1.png}
	\simg{q4c2.png}
	\simg{q4c3.png}
	
	All 3 plots imply that the regressor is correctly specified. For \uw{x}{7} specifically, it looks like the variance is a little higher on the right side, implying the variance isn't exactly constant, but it doesn't look like it changes the distribution, so it should still be good.

\newpage

\item Construct the partial regression plots for this model. Compare the plots with the plots of residuals versus regressors from part c above. Discuss the type of information provided by these plots.
\begin{verbatim}
model_wo_x2 = lm(y~x$x7+x$x8)
x2_tl_x7_x8 = lm(x$x2~x$x7+x$x8)
plot(resid(model_wo_x2)~resid(x2_tl_x7_x8), xlab = "x2 residuals regressed against x7, x8",
     ylab = "y residuals regressed against x7, x8")

model_wo_x7 = lm(y~x$x2+x$x8)
x7_tl_x2_x8 = lm(x$x7~x$x2+x$x8)
plot(resid(model_wo_x7)~resid(x7_tl_x2_x8), xlab = "x7 residuals regressed against x2, x8",
     ylab = "y residuals regressed against x2, x8")

model_wo_x8 = lm(y~x$x7+x$x2)
x8_tl_x7_x2 = lm(x$x8~x$x7+x$x2)
plot(resid(model_wo_x8)~resid(x8_tl_x7_x2), xlab = "x8 residuals regressed against x7, x2",
     ylab = "y residuals regressed against x7, x2")
\end{verbatim}

	\simg{q4d1.png}
	\simg{q4d2.png}
	\simg{q4d3.png}
	
	These plots tell us how much the error of one regressor affects the error of the whole model when taking into account the effect of other regressors on both the one regressor and the model itself.
	
\elist
}

\newpage

\bsc{Question 5}{
Show that the hat matrix H \eql X\uf{(X\pr X)}{-1}X\pr and I \ms H (where I is the identity matrix) are symmetric and idempotent. That is, please show:
\balist
\item H\pr \eql H and HH \eql H (H\pr\txt{ }means the transpose of H, HH means H * H)
	\eqn{
		\splt{
			H & = X\uf{(X\pr X)}{-1}X\pr \\
			H\pr & = (X\uf{(X\pr X)}{-1}X\pr)\pr \\
			& = X((X\pr X)^{-1})\pr X\pr \\
			& = X((X\pr X)\pr)^{-1}X\pr \\
			& = X(X\pr X)^{-1}X\pr \\
			& = H
		}
	}
	
	\eqn{
		\splt{
			H & = X\uf{(X\pr X)}{-1}X\pr \\
			HH & = (X\uf{(X\pr X)}{-1}X\pr)(X\uf{(X\pr X)}{-1}X\pr) \\
			HH & = X\uf{(X\pr X)}{-1}X\pr  X\uf{(X\pr X)}{-1}X\pr \\
			& = X\uf{(X\pr X)}{-1}X\pr \\
			& = H
		}
	}
\item (I \ms H)\pr \eql I \ms H and (I \ms H)(I \ms H) \eql I \ms H
	
	\eqn{
		\splt{
			(I - H)\pr & = (I - X\uf{(X\pr X)}{-1}X\pr)\pr \\
			& = I\pr - (X\uf{(X\pr X)}{-1}X\pr)\pr)\pr \\
			& = I - (X\uf{(X\pr X)}{-1}X\pr)\pr)\pr \\
			& = I - X\uf{(X\pr X)}{-1}X\pr \\
			& = I - H
		}
	}
	
	\eqn{
		\splt{
			(I - H)(I - H) & = (I - X\uf{(X\pr X)}{-1}X\pr)(I - X\uf{(X\pr X)}{-1}X\pr) \\
			& = I - 2X\uf{(X\pr X)}{-1}X\pr + (X\uf{(X\pr X)}{-1}X\pr)(X\uf{(X\pr X)}{-1}X\pr) \\
			& = I - 2X\uf{(X\pr X)}{-1}X\pr + X\uf{(X\pr X)}{-1}X\pr \txt{ by (a)} \\
			& = I - X\uf{(X\pr X)}{-1}X\pr \\
			& = I - H
		}
	}
\elist

Hint: A\eql X\pr X is a symmetric matrix, and for a symmetric matrix, \uf{(A\pr)}{-1} \eql (\uf{A}{-1})\pr. You can use this property directly in your proof of \bpth{a} and \bpth{b}. If you are interested in the proof of this property, you may check the following web page:

https://math.stackexchange.com/questions/325082/is-the-inverse-of-a-symmetric-matrix-also-symmetric

}
\end{document}