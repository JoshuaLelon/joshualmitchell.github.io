% Thank you Josh Davis for this template!
% https://github.com/jdavis/latex-homework-template/blob/master/homework.tex

\documentclass{article}

\newcommand{\hmwkTitle}{HW\ \#6}

% \input{ShortcutsStatistics}

% ----------

% Packages

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}
\usepackage{enumitem}
\usepackage{chngcntr}
\usepackage{blkarray}

% Libraries

\usetikzlibrary{automata, positioning, arrows}

%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{}
\rhead{\hmwkClass\ (\hmwkClassInstructor): \hmwkTitle}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}
\setcounter{secnumdepth}{0}

\newcommand{\hmwkClass}{MATH 5345 / Regression Analysis}        % Class
\newcommand{\hmwkClassInstructor}{Dr. Sun}           % Instructor
\newcommand{\hmwkAuthorName}{\textbf{Joshua Mitchell}} % Author

%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small\vspace{0.1in}\large{\textit{\hmwkClassInstructor}}
    \vspace{3in}
}

\author{\hmwkAuthorName}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

%
% Various Helper Commands
%

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}


% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Formatting commands:

\newcommand{\mt}[1]{\ensuremath{#1}}
\newcommand{\nm}[1]{\textrm{#1}}

\newcommand\bsc[2][\DefaultOpt]{%
  \def\DefaultOpt{#2}%
  \section[#1]{#2}%
}
\newcommand\ssc[2][\DefaultOpt]{%
  \def\DefaultOpt{#2}%
  \subsection[#1]{#2}%
}
\newcommand{\bgpf}{\begin{proof} $ $\newline}

\newcommand{\bgeq}{\begin{equation*}}
\newcommand{\eeq}{\end{equation*}}	

\newcommand{\balist}{\begin{enumerate}[label=\alph*.]}
\newcommand{\elist}{\end{enumerate}}

\newcommand{\bilist}{\begin{enumerate}[label=\roman*)]}	

\newcommand{\bgsp}{\begin{split}}
% \newcommand{\esp}{\end{split}} % doesn't work for some reason.

\newcommand\prs[1]{~~~\textbf{(#1)}}

\newcommand{\lt}[1]{\textbf{Let: } #1}
     							   %  if you're setting it to be true
\newcommand{\supp}[1]{\textbf{Suppose: } #1}
     							   %  Suppose (if it'll end up false)
\newcommand{\wts}[1]{\textbf{Want to show: } #1}
     							   %  Want to show
\newcommand{\as}[1]{\textbf{Assume: } #1}
     							   %  if you think it follows from truth
\newcommand{\bpth}[1]{\textbf{(#1)}}

\newcommand{\step}[2]{\begin{equation}\tag{#2}#1\end{equation}}
\newcommand{\epf}{\end{proof}}

\newcommand{\dbs}[3]{\mt{#1_{#2_#3}}}

\newcommand{\sidenote}[1]{-----------------------------------------------------------------Side Note----------------------------------------------------------------
#1 \

---------------------------------------------------------------------------------------------------------------------------------------------}

% Analysis / Logical commands:

\newcommand{\br}{\mt{\mathbb{R}} }       % |R
\newcommand{\bq}{\mt{\mathbb{Q}} }       % |Q
\newcommand{\bn}{\mt{\mathbb{N}} }       % |N
\newcommand{\bc}{\mt{\mathbb{C}} }       % |C
\newcommand{\bz}{\mt{\mathbb{Z}} }       % |Z

\newcommand{\ep}{\mt{\epsilon} }         % epsilon
\newcommand{\fa}{\mt{\forall} }          % for all
\newcommand{\afa}{\mt{\alpha} }
\newcommand{\bta}{\mt{\beta} }
\newcommand{\mem}{\mt{\in} }
\newcommand{\exs}{\mt{\exists} }

\newcommand{\es}{\mt{\emptyset} }        % empty set
\newcommand{\sbs}{\mt{\subset} }         % subset of
\newcommand{\fs}[2]{\{\uw{#1}{1}, \uw{#1}{2}, ... \uw{#1}{#2}\}}

\newcommand{\lra}{ \mt{\longrightarrow} } % implies ----->
\newcommand{\rar}{ \mt{\Rightarrow} }     % implies -->

\newcommand{\lla}{ \mt{\longleftarrow} }  % implies <-----
\newcommand{\lar}{ \mt{\Leftarrow} }      % implies <--

\newcommand{\av}[1]{\mt{|}#1\mt{|}}  % absolute value

\newcommand{\prn}[1]{(#1)}
\newcommand{\bk}[1]{\{#1\}}

\newcommand{\ps}{\mt{\operatorname{+}} }
\newcommand{\ms}{\mt{\operatorname{-}} }

\newcommand{\ls}{\mt{\operatorname{<}} }
\newcommand{\gr}{\mt{\operatorname{>}} }

\newcommand{\lse}{\mt{\operatorname{\leq}} }
\newcommand{\gre}{\mt{\operatorname{\geq}} }

\newcommand{\eql}{ \mt{\operatorname{=}} }

\newcommand{\pr}{\mt{^\prime}} 		   % prime (i.e. R')
\newcommand{\uw}[2]{#1\mt{_{#2}}}
\newcommand{\uf}[2]{#1\mt{^{#2}}}
\newcommand{\frc}[2]{\mt{\frac{#1}{#2}}}
\newcommand{\lmti}[1]{\mt{\displaystyle{\lim_{#1 \to \infty}}}}
\newcommand{\limt}[2]{\mt{\displaystyle{\lim_{#1 \to #2}}}}

\newcommand{\bnm}[2]{\mt{#1\setminus{#2}}}
\newcommand{\bnt}[2]{\mt{\textrm{#1}\setminus{\textrm{#2}}}}
\newcommand{\bi}{\bnm{\mathbb{R}}{\mathbb{Q}}}

\newcommand{\urng}[2]{\mt{\bigcup_{#1}^{#2}}}
\newcommand{\nrng}[2]{\mt{\bigcap_{#1}^{#2}}}
\newcommand{\nck}[2]{\mt{{#1 \choose #2}}}

\newcommand{\nbho}[3]{\textrm{N(}#1, #2\textrm{) }\cap \textrm{ #3} \neq \emptyset}
     							   %  N(x, eps) intersect S \= emptyset
\newcommand{\nbhe}[3]{\textrm{N(}#1, #2\textrm{) }\cap \textrm{ #3} = \emptyset}
     							   %  N(x, eps) intersect S  = emptyset
\newcommand{\dnbho}[3]{\textrm{N*(}#1, #2\textrm{) }\cap \textrm{ #3} \neq \emptyset}
     							   %  N*(x, eps) intersect S \= emptyset
\newcommand{\dnbhe}[3]{\textrm{N*(}#1, #2\textrm{) }\cap \textrm{ #3} = \emptyset}
     							   %  N*(x, eps) intersect S = emptyset
     							   
\newcommand{\eqn}[1]{\[#1\]}
\newcommand{\splt}[1]{\begin{split}#1\end{split}}

\newcommand{\txt}[1]{\text{#1}} % Not new command, but remember \text for text in eqns
\newcommand{\tl}{\mt{\thicksim}}
\newcommand{\mn}[1]{\mt{\overline{#1}}}
\newcommand{\sg}{\mt{\sigma}}
\newcommand{\ssq}{\mt{\sigma^2}}	

\newcommand{\bh}[1]{\mathbf{\hat{\text{$#1$}}}}
\newcommand{\bth}{\mt{\bh{\beta}}}
\newcommand{\yh}{\mt{\bh{Y}}}

\newcommand{\exv}[1]{\txt{E[}#1\txt{]}}
\newcommand{\vrn}[1]{V[#1]}

\newcommand{\gv}{ \mt{|} }

\newcommand{\cov}[2]{\txt{Cov(#1, #2)}}

\newcommand{\img}[1]{
\begin{figure}[h]
  \includegraphics[width=0.5\linewidth]{#1}
\end{figure}
}
\newcommand{\simg}[1]{
  \includegraphics[width=0.35\linewidth]{#1}
}
\newcommand{\wimg}[1]{
\begin{figure}[h]
  \includegraphics[width=1\linewidth]{#1}
\end{figure}
}
\newcommand\tab[1][1cm]{\hspace*{#1}}	
\newcommand{\sumin}[1]{\mt{\sum_{i = 1}^n #1}}	

\newcommand{\brm}[1]{\begin{pmatrix} #1 \end{pmatrix}}

\newcommand{\inm}[1]{\mt{\left\[ \begin{smallmatrix} #1 \end{smallmatrix} \right\]}}

\newcommand{\lbm}[4]{
	  \begin{blockarray}{#1} % a c for every row, plus the c-labels
        #2 \\ % & c-label1 & c-label2 & c-label3...
      \begin{block}{c(#3)} % a c for every column only
        #4 % r-label1 & data & data & data ... \\
           % r-label2 & data & data & data ... \\
           % ...
           % r-labeln & data & data & data ... \\
      \end{block}
    \end{blockarray}
}

% Example:
% \[\lbm{ccccc}{& H & y & d}{cccc}{H & 4 & 4 & 4 \\
% Y & 3 & 3 & 3 \\
% D & 2 & 2 & 2 \\
% D & 2 & 2 & 2 \\}\]

\newcommand{\unds}[2]{\mt{\underset{#1}{#2}}} % stuff underneath!	 
% ----------

\begin{document}
\begin{table}[ht]
\centering
\begin{tabular}{llllll}
  \hline
 & Q1 & Q2 & Q3 & Q4 & Q5 \\ 
  \hline
50 Points & 10 & 10 & 10 & 10 & 10 \\ 
   &  &  &  &  &  \\ 
   \hline
\end{tabular}
\end{table}
\bsc{Question 1}{
\wimg{HW6Q1}
\ssc{Q1 Part (a)}{

The five key assumptions:

\balist
\item Normality - Our response variable(s) (by themselves), residuals (by themselves), and residuals vs regressors, look normal.

\simg{HW6Q1A1}
\simg{HW6Q1A2}

\textbf{The distribution of our response variable seems mostly normal - I don't know what to make of the gap in the middle, though.}

\item Independence - Our samples are independent (i.e. the value of one does not affect the value of any other). This is usually the hardest one to test for - usually it's argued from a sampling side. If you plot the residuals vs the predicted values, if they're independent, you should see no pattern.

\simg{HW6Q1X1}
\simg{HW6Q1X2}
\simg{HW6Q1X3}

\textbf{Our samples appear to be independent - there doesn't seem to be a pattern in the data (but then again, there's only 13 data points).}

\item Constant Variance - The residual plots should just be bands (i.e. no funnels, cones, or any weird shape).

\textbf{It does appear that we have a "bowtie" kind of pattern, so I would assume that we don't have constant variance.}

\item \exv{$\epsilon$} \eql 0 - This is assumed since that's the way we build our model (i.e. via least squares)

\textbf{The cluster of residuals for all the residual plots seems to be centered around 0.}

\item Linearity - The model actually fits (i.e. the data follows the shape of the model: R$^2$ is high)

\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 27.7692 & 5.3190 & 5.22 & 0.0008 \\ 
  as.factor(x1)0 & 0.7308 & 6.5577 & 0.11 & 0.9140 \\ 
  as.factor(x1)1 & 31.8462 & 6.7280 & 4.73 & 0.0015 \\ 
  as.factor(x2)1 & -8.4615 & 6.2935 & -1.34 & 0.2157 \\ 
  as.factor(x3)1 & -9.9615 & 6.2935 & -1.58 & 0.1521 \\ 
   \hline
\end{tabular}
\end{table}

Multiple R-squared:  0.7393, 	Adjusted R-squared:  0.609

F-statistic: 5.672 on 4 and 8 DF,  p-value: 0.01828

Only the intercept (***) and as.factor(x1)0 (**) are significant.

\textbf{I would say the model somewhat fits - it's hard to say since we only have 13 data points. Our R$^2$ is mediocre, and we have a lot of degrees of freedom relative to our number of samples.}

\textbf{Technically, if we do a hypothesis test and assume \uw{H}{0}: all \bta's are 0, then we can disprove the null hypothesis with our \bta for \uw{x}{1}'s 2nd factor (as shown in the table: as.factor(x1)1). But, everything else is insignificant. I'd say the model fits, but barely (needs changes).}

\elist

}
\ssc{Q1 Part (b)}{

Data:

\begin{table}[ht]
\centering
\begin{tabular}{rrrrrr}
  \hline
 & x1 & x2 & x3 & y & level \\ 
  \hline
1 &  -1 &  -1 &  -1 & 17.00 & 1 \\ 
  2 &  1 &  -1 &  -1 & 44.00 & 2 \\ 
  3 &  -1 &   1 &  -1 & 19.00 & 3 \\ 
  4 &   1 &   1 &  -1 & 46.00 & 4 \\ 
  5 &  -1 &  -1 &   1 & 7.00 & 5 \\ 
  6 &   1 &  -1 &   1 & 55.00 & 6 \\ 
  7 &  -1 &   1 &   1 & 15.00 & 7 \\ 
  8 &   1 &   1 &   1 & 41.00 & 8 \\ 
  9 &   0 &   0 &   0 & 29.00 & 9 \\ 
  10 &   0 &   0 &   0 & 28.50 & 9 \\ 
  11 &   0 &   0 &   0 & 30.00 & 9 \\ 
  12 &   0 &   0 &   0 & 27.00 & 9 \\ 
  13 &   0 &   0 &   0 & 28.00 & 9 \\ 
   \hline
\end{tabular}
\end{table}

Recall:

\uw{SS}{Res} \eql \uw{SS}{PE} \ps \uw{SS}{LOF}

Our test statistic is:
\eqn{F_0 = \frac{SS_{LOF} / (m - p)}{SS_{PE} / (n - m)} = \frac{MS_{LOF}}{MS_{PE}}}

If \uw{F}{0} \gr \uw{F}{m \ms p, n \ms m}, conclude that the regression function is not linear.

From R:

\begin{table}[ht]
\centering
\begin{tabular}{lrrrrr}
  \hline
 & Df & Sum Sq & Mean Sq & F value & Pr($>$F) \\ 
  \hline
as.factor(x1) & 2 & 2060.31 & 1030.15 & 824.12 & 0.0000 \\ 
  as.factor(x2) & 1 & 0.50 & 0.50 & 0.40 & 0.5614 \\ 
  as.factor(x3) & 1 & 8.00 & 8.00 & 6.40 & 0.0647 \\ 
  Residuals & 8 & 188.50 & 23.56 &  &  \\ 
   Lack of fit & 4 & 183.50 & 45.87 & 36.70 & 0.0021 \\ 
   Pure Error & 4 & 5.00 & 1.25 &  &  \\ 
   \hline
\end{tabular}
\end{table}

\eqn{m = 9}
\eqn{(29 + 28.5 + 30 + 27 + 28) / 5 = 28.5}
\eqn{(29 - 28.5)^2 + (28.5 - 28.5)^2 + (30 - 28.5)^2 + (27 - 28.5)^2 + (28 - 28.5)^2 \eql 5 (SS_{PE})}
\eqn{188.50 = 5 + SS_{LOF} \lra SS_{LOF} = 183.50}
\eqn{F_0 = \frac{183.50 / (9 - 5)}{5 / (13 - 9)} = \frac{45.875}{1.25} = 36.70}

Test:

\eqn{36.70 > \uw{F}{m \ms p, n \ms m} = F_{4, 4} = 4.10725 \textrm{ (even at \afa \eql 0.10 it still fails)}}

\textbf{So we conclude that the regression function is not linear.}

}

}

\newpage

\bsc{Question 2}{
\wimg{HW6Q2}
\ssc{Q2 Part (a)}{
\simg{HW6Q2ScPlt}

\textbf{Yes, it seems likely that a straight line model will be adequate.}
}
\ssc{Q2 Part (b)}{

\begin{verbatim}
q2lm <- lm(q2data$visc ~ q2data$temp)
summary(q2lm)
plot(q2lm$fitted.values, resid(q2lm))
abline(h = 0, col="red")
\end{verbatim}


\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 1.2815 & 0.0469 & 27.34 & 0.0000 \\ 
  q2data\$temp & -0.0088 & 0.0007 & -12.02 & 0.0000 \\ 
   \hline
\end{tabular}
\end{table}

Residual standard error: 0.04743 on 6 degrees of freedom

Multiple R-squared:  0.9602 \tab Adjusted R-squared:  0.9535

\simg{HW6Q2PBPlt}

\textbf{It looks like all 5 of our assumptions hold except for non constant variance for our error term (due to the quadratic pattern).}
}

\newpage

\ssc{Q2 Part (c)}{

\begin{verbatim}
q2lm <- lm(q2data$visc ~ q2data$temp + exp(q2data$temp))
summary(q2lm)
plot(q2lm$fitted.values, resid(q2lm))
plot(q2data$temp, resid(q2lm))
plot(exp(q2data$temp), resid(q2lm))
\end{verbatim}

\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 1.3195 & 0.0424 & 31.11 & 0.0000 \\ 
  q2data\$temp & -0.0096 & 0.0007 & -13.27 & 0.0000 \\ 
  exp(q2data\$temp) & 0.0000 & 0.0000 & 2.03 & 0.0986 \\ 
   \hline
\end{tabular}
\end{table}

\simg{HW6Q2PC1}
\simg{HW6Q2PC2}
\simg{HW6Q2PC3}

\textbf{Post-Transformation:}

\begin{verbatim}
q2lm_t <- lm(log(q2data$visc) ~ q2data$temp + exp(q2data$temp))
summary(q2lm_t)
plot(q2lm_t$fitted.values, resid(q2lm_t), main="[Transformed] Residuals vs Fitted", xlab="Fitted", ylab="Residuals")
plot(q2data$temp, resid(q2lm_t), main="[Transformed] Residuals vs Temp", xlab="Temp", ylab="Residuals")
plot(exp(q2data$temp), resid(q2lm_t), main="[Transformed] Residuals vs e^(temp)", xlab="e^(temp)", ylab="Residuals")
\end{verbatim}

\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 0.4036 & 0.0192 & 20.99 & 0.0000 \\ 
  q2data\$temp & -0.0121 & 0.0003 & -37.02 & 0.0000 \\ 
  exp(q2data\$temp) & 0.0000 & 0.0000 & 3.23 & 0.0232 \\ 
   \hline
\end{tabular}
\end{table}

\simg{HW6Q2PC4}
\simg{HW6Q2PC5}
\simg{HW6Q2PC6}

\textbf{Looks like we still have non-constant variance, but the p values and R$^2$ got better after both the additional \bta and the log transformation.}

}
}

\newpage

\bsc{Question 3}{
\wimg{HW6Q3}

\ssc{Q3 Part (a)}{

\simg{HW6Q3PAHist}
\simg{HW6Q3PARes}

\simg{HW6Q3PA}
\simg{HW6Q3R4}

\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 5.8945 & 4.3251 & 1.36 & 0.1783 \\ 
  x1 & -0.4779 & 0.3400 & -1.41 & 0.1653 \\ 
  x2 & 0.1827 & 0.0172 & 10.63 & 0.0000 \\ 
  x3 & 35.4028 & 11.0996 & 3.19 & 0.0023 \\ 
  x4 & 5.8439 & 2.9098 & 2.01 & 0.0494 \\ 
   \hline
\end{tabular}
\end{table}

\begin{verbatim}
  Multiple R-squared:  0.6914,		Adjusted R-squared:  0.6697 
\end{verbatim}

\newpage

The five key assumptions:

\balist
\item Normality - \textbf{Looks somewhat normal, maybe with a long right tail.}

\item Independence - \textbf{Since there doesn't appear to be any pattern in the residuals vs fitted values, I would say the errors are independent.}

\item Constant Variance - \textbf{It looks okay, but there might be a slight quadratic pattern.}

\item \exv{$\epsilon$} \eql 0 - \textbf{This is assumed since that's the way we build our model (i.e. via least squares).}

\item Linearity - Multiple R-squared:  0.6914 \tab 	Adjusted R-squared:  0.6697

\textbf{\uw{x}{2} (***), \uw{x}{3} (**), and \uw{x}{4} (*) are all significant, so if \uw{H}{0} is that there is no linear relationship between any of our regressors and our response variable, then we reject \uw{H}{0}.}

\elist

}

\ssc{Q3 Part (b)}{

\simg{HW6Q3PBHist}
\simg{HW6Q3PBRF}

\simg{HW6Q3PBQQPlot}
\simg{HW6Q3PBR4}

\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 2.2285 & 0.2033 & 10.96 & 0.0000 \\ 
  x1 & -0.0162 & 0.0160 & -1.01 & 0.3159 \\ 
  x2 & 0.0066 & 0.0008 & 8.13 & 0.0000 \\ 
  x3 & 1.8502 & 0.5218 & 3.55 & 0.0008 \\ 
  x4 & 0.2548 & 0.1368 & 1.86 & 0.0677 \\ 
   \hline
\end{tabular}
\end{table}

Multiple R-squared:  0.5894 \tab Adjusted R-squared:  0.5606

\textbf{Oddly enough, it looks like we improved some aspects of our model (our y looks more normal and our residuals look a little better, but our R$^2$ went down significantly).}
}

\newpage

\ssc{Q3 Part (c): Pre-Log Transformation, Influential Points Removed}{

\simg{HW6Q3PCRegHist}
\simg{HW6Q3PCRegResFitted}

\simg{HW6Q3PCRegQQ}
\simg{HW6Q3PCRegResX4}

\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & -13.1146 & 9.4792 & -1.38 & 0.1728 \\ 
  x1 & 0.0268 & 0.3511 & 0.08 & 0.9395 \\ 
  x2 & 0.1984 & 0.0192 & 10.35 & 0.0000 \\ 
  x3 & 87.5725 & 27.1116 & 3.23 & 0.0022 \\ 
  x4 & 4.2155 & 2.8757 & 1.47 & 0.1491 \\ 
   \hline
\end{tabular}
\end{table}

\begin{verbatim}
  Multiple R-squared:  0.7178,	 Adjusted R-squared:  0.6947
\end{verbatim}

\begin{table}[ht]
\centering
\begin{tabular}{lrrrrr}
  \hline
 & Df & Sum Sq & Mean Sq & F value & Pr($>$F) \\ 
  \hline
x1 & 1 & 20.96 & 20.96 & 0.97 & 0.3291 \\ 
  x2 & 1 & 2402.11 & 2402.11 & 111.37 & 0.0000 \\ 
  x3 & 1 & 218.45 & 218.45 & 10.13 & 0.0025 \\ 
  x4 & 1 & 46.35 & 46.35 & 2.15 & 0.1491 \\ 
  Residuals & 49 & 1056.90 & 21.57 &  &  \\ 
   \hline
\end{tabular}
\end{table}

\textbf{Influential Points (by data sample index):}

22 50 51 52 53 54 56 59
}

\newpage

\ssc{Q3 Part (c): Post-Log Transformation, Influential Points Removed}{

\simg{HW6Q3PCLogHist}
\simg{HW6Q3PCLogResFitted}

\simg{HW6Q3PCLogQQPlot}
\simg{HW6Q3PCLogResX4}

\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 2.2988 & 0.3211 & 7.16 & 0.0000 \\ 
  x1 & -0.0128 & 0.0172 & -0.75 & 0.4591 \\ 
  x2 & 0.0057 & 0.0011 & 5.04 & 0.0000 \\ 
  x3 & 1.6190 & 0.8849 & 1.83 & 0.0735 \\ 
  x4 & 0.2822 & 0.1371 & 2.06 & 0.0450 \\ 
   \hline
\end{tabular}
\end{table}

\begin{verbatim}
  Multiple R-squared:  0.4198, 	Adjusted R-squared:  0.3715
\end{verbatim}

\begin{table}[ht]
\centering
\begin{tabular}{lrrrrr}
  \hline
 & Df & Sum Sq & Mean Sq & F value & Pr($>$F) \\ 
  \hline
x1 & 1 & 0.00 & 0.00 & 0.00 & 0.9613 \\ 
  x2 & 1 & 1.47 & 1.47 & 27.53 & 0.0000 \\ 
  x3 & 1 & 0.16 & 0.16 & 2.96 & 0.0918 \\ 
  x4 & 1 & 0.23 & 0.23 & 4.24 & 0.0450 \\ 
  Residuals & 48 & 2.56 & 0.05 &  &  \\ 
   \hline
\end{tabular}
\end{table}

\textbf{Influential Points (by data sample index):}

35 50 51 52 54 57 59 60 62

\textbf{Influential Points in both the pre and post transformed models:}

50 51 52 54 59

\textbf{Conclusion:}

Transforming the data via log(y) actually does worse in both cases (with and without influential points). It turns out that the transformed model without influential points had the worst $R^2$ out of the four, but the original model without influential points had the best $R^2$ out of the four. Transforming the data overall tends to improve how normally distributed the predictor is (and slightly improve the consistency of the variance of the error term), but still makes the model worse in terms of $R^2$.

}
}

\newpage

\bsc{Question 4}{
\wimg{HW6Q4}

\ssc{Q4 Part (a)}{

\simg{HW6Q4LM}

\textbf{It fits okay. Could be better. R$^2$ \eql 0.9038}

}
\ssc{Q4 Part (b)}{

\simg{HW6Q4PB}

\textbf{It suggests that, since there's a slight curve, perhaps y isn't linear.}

}
\ssc{Q4 Part (c)}{

\simg{HW6Q4PC}

\textbf{It looks like we have non constant variance (due to the quadratic shape).}
}
\ssc{Q4 Part (d)}{

\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 20.1000 & 6.3360 & 3.17 & 0.0504 \\ 
  x & -1.4696 & 0.4145 & -3.55 & 0.0382 \\ 
  I(x\verb|^|2) & 0.0598 & 0.0058 & 10.31 & 0.0019 \\ 
   \hline
\end{tabular}
\end{table}

\textbf{Yes, the x$^2$ term is statistically significant (**).}

}
\ssc{Q4 Part (e)}{

\simg{HW6Q4PE1}
\simg{HW6Q4PE2}
\simg{HW6Q4PE3}

\textbf{Yes, there is evidence it is a better fit. The non-constant variance problem seems to be reduced (although you could argue that there is still a slight quadratic shape, but there's not enough data to really say either way). The R$^2$ is also a lot better: 0.9974} 

}
}

\newpage

\bsc{Question 5}{
\wimg{HW6Q5}
\ssc{Q5 Part (a)}{

\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 33.6184 & 1.5395 & 21.84 & 0.0000 \\ 
  x1 & -0.0457 & 0.0087 & -5.27 & 0.0000 \\ 
  as.factor(x11)1 & -0.4987 & 2.2282 & -0.22 & 0.8245 \\ 
   \hline
\end{tabular}
\end{table}

\textbf{The p value of x11 in the model is 0.8245, so there isn't significant evidence to conclude that there is a linear relationship between y and x11.}

}
\ssc{Q5 Part (b)}{

\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 42.9196 & 2.7349 & 15.69 & 0.0000 \\ 
  x1 & -0.1168 & 0.0198 & -5.89 & 0.0000 \\ 
  as.factor(x11)1 & -13.4637 & 3.8441 & -3.50 & 0.0016 \\ 
  x1:as.factor(x11)1 & 0.0816 & 0.0213 & 3.84 & 0.0006 \\ 
   \hline
\end{tabular}
\end{table}

\textbf{x1 (***), x11 (**), and x1:x11(***) are significant. It looks like the type of transmission (x11) has a significant effect on the gasoline mileage (y) when you account for the interaction between the engine displacement (x1) and type of transmission (x11).}

}
}
\end{document}