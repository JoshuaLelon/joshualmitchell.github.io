% Thank you Josh Davis for this template!
% https://github.com/jdavis/latex-homework-template/blob/master/homework.tex

\documentclass{article}

\newcommand{\hmwkTitle}{HW\ \#6}

% \input{ShortcutsStatistics}

% ----------

% Packages

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}
\usepackage{enumitem}
\usepackage{chngcntr}
\usepackage{blkarray}

% Libraries

\usetikzlibrary{automata, positioning, arrows}

%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{}
\rhead{\hmwkClass\ (\hmwkClassInstructor): \hmwkTitle}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}
\setcounter{secnumdepth}{0}

\newcommand{\hmwkClass}{MATH 5345 / Regression Analysis}        % Class
\newcommand{\hmwkClassInstructor}{Dr. Sun}           % Instructor
\newcommand{\hmwkAuthorName}{\textbf{Joshua Mitchell}} % Author

%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small\vspace{0.1in}\large{\textit{\hmwkClassInstructor}}
    \vspace{3in}
}

\author{\hmwkAuthorName}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

%
% Various Helper Commands
%

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}


% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Formatting commands:

\newcommand{\mt}[1]{\ensuremath{#1}}
\newcommand{\nm}[1]{\textrm{#1}}

\newcommand\bsc[2][\DefaultOpt]{%
  \def\DefaultOpt{#2}%
  \section[#1]{#2}%
}
\newcommand\ssc[2][\DefaultOpt]{%
  \def\DefaultOpt{#2}%
  \subsection[#1]{#2}%
}
\newcommand{\bgpf}{\begin{proof} $ $\newline}

\newcommand{\bgeq}{\begin{equation*}}
\newcommand{\eeq}{\end{equation*}}	

\newcommand{\balist}{\begin{enumerate}[label=\alph*.]}
\newcommand{\elist}{\end{enumerate}}

\newcommand{\bilist}{\begin{enumerate}[label=\roman*)]}	

\newcommand{\bgsp}{\begin{split}}
% \newcommand{\esp}{\end{split}} % doesn't work for some reason.

\newcommand\prs[1]{~~~\textbf{(#1)}}

\newcommand{\lt}[1]{\textbf{Let: } #1}
     							   %  if you're setting it to be true
\newcommand{\supp}[1]{\textbf{Suppose: } #1}
     							   %  Suppose (if it'll end up false)
\newcommand{\wts}[1]{\textbf{Want to show: } #1}
     							   %  Want to show
\newcommand{\as}[1]{\textbf{Assume: } #1}
     							   %  if you think it follows from truth
\newcommand{\bpth}[1]{\textbf{(#1)}}

\newcommand{\step}[2]{\begin{equation}\tag{#2}#1\end{equation}}
\newcommand{\epf}{\end{proof}}

\newcommand{\dbs}[3]{\mt{#1_{#2_#3}}}

\newcommand{\sidenote}[1]{-----------------------------------------------------------------Side Note----------------------------------------------------------------
#1 \

---------------------------------------------------------------------------------------------------------------------------------------------}

% Analysis / Logical commands:

\newcommand{\br}{\mt{\mathbb{R}} }       % |R
\newcommand{\bq}{\mt{\mathbb{Q}} }       % |Q
\newcommand{\bn}{\mt{\mathbb{N}} }       % |N
\newcommand{\bc}{\mt{\mathbb{C}} }       % |C
\newcommand{\bz}{\mt{\mathbb{Z}} }       % |Z

\newcommand{\ep}{\mt{\epsilon} }         % epsilon
\newcommand{\fa}{\mt{\forall} }          % for all
\newcommand{\afa}{\mt{\alpha} }
\newcommand{\bta}{\mt{\beta} }
\newcommand{\mem}{\mt{\in} }
\newcommand{\exs}{\mt{\exists} }

\newcommand{\es}{\mt{\emptyset} }        % empty set
\newcommand{\sbs}{\mt{\subset} }         % subset of
\newcommand{\fs}[2]{\{\uw{#1}{1}, \uw{#1}{2}, ... \uw{#1}{#2}\}}

\newcommand{\lra}{ \mt{\longrightarrow} } % implies ----->
\newcommand{\rar}{ \mt{\Rightarrow} }     % implies -->

\newcommand{\lla}{ \mt{\longleftarrow} }  % implies <-----
\newcommand{\lar}{ \mt{\Leftarrow} }      % implies <--

\newcommand{\av}[1]{\mt{|}#1\mt{|}}  % absolute value

\newcommand{\prn}[1]{(#1)}
\newcommand{\bk}[1]{\{#1\}}

\newcommand{\ps}{\mt{\operatorname{+}} }
\newcommand{\ms}{\mt{\operatorname{-}} }

\newcommand{\ls}{\mt{\operatorname{<}} }
\newcommand{\gr}{\mt{\operatorname{>}} }

\newcommand{\lse}{\mt{\operatorname{\leq}} }
\newcommand{\gre}{\mt{\operatorname{\geq}} }

\newcommand{\eql}{ \mt{\operatorname{=}} }

\newcommand{\pr}{\mt{^\prime}} 		   % prime (i.e. R')
\newcommand{\uw}[2]{#1\mt{_{#2}}}
\newcommand{\uf}[2]{#1\mt{^{#2}}}
\newcommand{\frc}[2]{\mt{\frac{#1}{#2}}}
\newcommand{\lmti}[1]{\mt{\displaystyle{\lim_{#1 \to \infty}}}}
\newcommand{\limt}[2]{\mt{\displaystyle{\lim_{#1 \to #2}}}}

\newcommand{\bnm}[2]{\mt{#1\setminus{#2}}}
\newcommand{\bnt}[2]{\mt{\textrm{#1}\setminus{\textrm{#2}}}}
\newcommand{\bi}{\bnm{\mathbb{R}}{\mathbb{Q}}}

\newcommand{\urng}[2]{\mt{\bigcup_{#1}^{#2}}}
\newcommand{\nrng}[2]{\mt{\bigcap_{#1}^{#2}}}
\newcommand{\nck}[2]{\mt{{#1 \choose #2}}}

\newcommand{\nbho}[3]{\textrm{N(}#1, #2\textrm{) }\cap \textrm{ #3} \neq \emptyset}
     							   %  N(x, eps) intersect S \= emptyset
\newcommand{\nbhe}[3]{\textrm{N(}#1, #2\textrm{) }\cap \textrm{ #3} = \emptyset}
     							   %  N(x, eps) intersect S  = emptyset
\newcommand{\dnbho}[3]{\textrm{N*(}#1, #2\textrm{) }\cap \textrm{ #3} \neq \emptyset}
     							   %  N*(x, eps) intersect S \= emptyset
\newcommand{\dnbhe}[3]{\textrm{N*(}#1, #2\textrm{) }\cap \textrm{ #3} = \emptyset}
     							   %  N*(x, eps) intersect S = emptyset
     							   
\newcommand{\eqn}[1]{\[#1\]}
\newcommand{\splt}[1]{\begin{split}#1\end{split}}

\newcommand{\txt}[1]{\text{#1}} % Not new command, but remember \text for text in eqns
\newcommand{\tl}{\mt{\thicksim}}
\newcommand{\mn}[1]{\mt{\overline{#1}}}
\newcommand{\sg}{\mt{\sigma}}
\newcommand{\ssq}{\mt{\sigma^2}}	

\newcommand{\bh}[1]{\mathbf{\hat{\text{$#1$}}}}
\newcommand{\bth}{\mt{\bh{\beta}}}
\newcommand{\yh}{\mt{\bh{Y}}}

\newcommand{\exv}[1]{\txt{E[}#1\txt{]}}
\newcommand{\vrn}[1]{V[#1]}

\newcommand{\gv}{ \mt{|} }

\newcommand{\cov}[2]{\txt{Cov(#1, #2)}}

\newcommand{\img}[1]{
\begin{figure}[h]
  \includegraphics[width=0.5\linewidth]{#1}
\end{figure}
}
\newcommand{\simg}[1]{
  \includegraphics[width=0.35\linewidth]{#1}
}
\newcommand{\wimg}[1]{
\begin{figure}[h]
  \includegraphics[width=1\linewidth]{#1}
\end{figure}
}
\newcommand\tab[1][1cm]{\hspace*{#1}}	
\newcommand{\sumin}[1]{\mt{\sum_{i = 1}^n #1}}	

\newcommand{\brm}[1]{\begin{pmatrix} #1 \end{pmatrix}}

\newcommand{\inm}[1]{\mt{\left\[ \begin{smallmatrix} #1 \end{smallmatrix} \right\]}}

\newcommand{\lbm}[4]{
	  \begin{blockarray}{#1} % a c for every row, plus the c-labels
        #2 \\ % & c-label1 & c-label2 & c-label3...
      \begin{block}{c(#3)} % a c for every column only
        #4 % r-label1 & data & data & data ... \\
           % r-label2 & data & data & data ... \\
           % ...
           % r-labeln & data & data & data ... \\
      \end{block}
    \end{blockarray}
}

% Example:
% \[\lbm{ccccc}{& H & y & d}{cccc}{H & 4 & 4 & 4 \\
% Y & 3 & 3 & 3 \\
% D & 2 & 2 & 2 \\
% D & 2 & 2 & 2 \\}\]

\newcommand{\unds}[2]{\mt{\underset{#1}{#2}}} % stuff underneath!	 
% ----------

\begin{document}
\begin{table}[ht]
\centering
\begin{tabular}{llllll}
  \hline
 & Q1 & Q2 & Q3 & Q4 & Q5 \\ 
  \hline
50 Points & 10 & 10 & 10 & 10 & 10 \\ 
   &  &  &  &  &  \\ 
   \hline
\end{tabular}
\end{table}
\bsc{Question 1}{
\wimg{HW6Q1}
\ssc{Q1 Part (a)}{

The five key assumptions:

\balist
\item Normality - Our response variable(s) (by themselves), residuals (by themselves), and residuals vs regressors, when histogrammed, look normal.

\simg{HW6Q1A1}

\textbf{The distribution of our response variable seems mostly normal - I don't know what to make of the gap in the middle, though.}

\item Independence - Our samples are independent (i.e. the value of one does not affect the value of any other). This is usually the hardest one to test for - usually it's argued from a sampling side. If you plot the residuals vs the predicted values, if they're independent, you should see no pattern.

\simg{HW6Q1X1}
\simg{HW6Q1X2}
\simg{HW6Q1X3}

\textbf{Our samples appear to be independent - there doesn't seem to be a pattern in the data (but then again, there's only 13 data points).}

\item Constant Variance - The residual plots should just be bands (i.e. no funnels, cones, or any weird shape).

\textbf{It does appear that we have a "bowtie" kind of pattern, so I would assume that we don't have constant variance.}

\item \exv{$\epsilon$} \eql 0 - This is assumed since that's the way we build our model (i.e. via least squares)

\textbf{Since we made our model this same way, I think it's valid to say we have \exv{$\epsilon$} \eql 0.}

\item Linearity - The model actually fits (i.e. the data follows the shape of the model: R$^2$ is high)

\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 27.7692 & 5.3190 & 5.22 & 0.0008 \\ 
  as.factor(x1)0 & 0.7308 & 6.5577 & 0.11 & 0.9140 \\ 
  as.factor(x1)1 & 31.8462 & 6.7280 & 4.73 & 0.0015 \\ 
  as.factor(x2)1 & -8.4615 & 6.2935 & -1.34 & 0.2157 \\ 
  as.factor(x3)1 & -9.9615 & 6.2935 & -1.58 & 0.1521 \\ 
   \hline
\end{tabular}
\end{table}

Multiple R-squared:  0.7393, 	Adjusted R-squared:  0.609

F-statistic: 5.672 on 4 and 8 DF,  p-value: 0.01828

Only the intercept (***) and as.factor(x1)0 (**) are significant.

\textbf{I would say the model somewhat fits - it's hard to say since we only have 13 data points. Our R$^2$ is mediocre, and we have a lot of degrees of freedom relative to our number of samples.}

\textbf{Technically, if we do a hypothesis test and assume \uw{H}{0}: all \bta's are 0, then we can disprove the null hypothesis with our \bta for \uw{x}{1}'s 2nd factor (as shown in the table: as.factor(x1)1). But, everything else is insignificant. I'd say the model fits, but barely (needs changes).}

\elist

}
\ssc{Q1 Part (b)}{

Data:

\begin{table}[ht]
\centering
\begin{tabular}{rrrrrr}
  \hline
 & x1 & x2 & x3 & y & level \\ 
  \hline
1 &  -1 &  -1 &  -1 & 17.00 & 1 \\ 
  2 &  -1 &  -1 &  -1 & 44.00 & 1 \\ 
  3 &  -1 &   1 &  -1 & 19.00 & 2 \\ 
  4 &   1 &   1 &  -1 & 46.00 & 3 \\ 
  5 &  -1 &  -1 &   1 & 7.00 & 4 \\ 
  6 &   1 &  -1 &   1 & 55.00 & 5 \\ 
  7 &  -1 &   1 &   1 & 15.00 & 6 \\ 
  8 &   1 &   1 &   1 & 41.00 & 7 \\ 
  9 &   0 &   0 &   0 & 29.00 & 8 \\ 
  10 &   0 &   0 &   0 & 28.50 & 8 \\ 
  11 &   0 &   0 &   0 & 30.00 & 8 \\ 
  12 &   0 &   0 &   0 & 27.00 & 8 \\ 
  13 &   0 &   0 &   0 & 28.00 & 8 \\ 
   \hline
\end{tabular}
\end{table}

Recall:

\uw{SS}{Res} \eql \uw{SS}{PE} \ps \uw{SS}{LOF}

Our test statistic is:
\eqn{F_0 = \frac{SS_{LOF} / (m - 2)}{SS_{PE} / (n - m)} = \frac{MS_{LOF}}{MS_{PE}}}

If \uw{F}{0} \gr \uw{F}{m \ms 2, n \ms m}, conclude that the regression function is not linear.

From R:

\begin{table}[ht]
\centering
\begin{tabular}{lrrrrr}
  \hline
 & Df & Sum Sq & Mean Sq & F value & Pr($>$F) \\ 
  \hline
as.factor(x1) & 2 & 1372.44 & 686.22 & 9.33 & 0.0081 \\ 
  as.factor(x2) & 1 & 112.12 & 112.12 & 1.52 & 0.2520 \\ 
  as.factor(x3) & 1 & 184.29 & 184.29 & 2.51 & 0.1521 \\ 
  Residuals & 8 & 588.46 & 73.56 &  &  \\ 
   \hline
\end{tabular}
\end{table}

Since we have 8 levels of (\uw{x}{1}, \uw{x}{2}, \uw{x}{3}),
\eqn{m = 8}
\eqn{(29 + 28.5 + 30 + 27 + 28) / 5 = 28.5 = ??}
\eqn{(17 + 44) / 2 = 30.5 = ??}
\eqn{(29 - 28.5)^2 + (28.5 - 28.5)^2 + (30 - 28.5)^2 + (27 - 28.5)^2 + (28 - 28.5)^2 \eql 5 = ?? (SS_{PE}?)}
\eqn{(17 - 30.5)^2 + (44 - 30.5)^2 = 364.5 = ??}
\eqn{588.46 = 5 + SS_{LOF} \lra SS_{LOF} = 583.46}
\eqn{F_0 = \frac{583.46 / (8 - 2)}{5 / (13 - 8)} = \frac{194.486666667}{0.625} = 311.178666667}

Test:

\eqn{311.178666667 > \uw{F}{m \ms 2, n \ms m} = F_{3, 8} = 2.92380 \textrm{ (even at \afa \eql 0.10 it still fails)}}

\textbf{So we conclude that the regression function is not linear.}

}

}

\newpage

\bsc{Question 2}{
\wimg{HW6Q2}
\ssc{Q2 Part (a)}{
\simg{HW6Q2ScPlt}

\textbf{Yes, it seems likely that a straight line model will be adequate.}
}
\ssc{Q2 Part (b)}{

\begin{verbatim}
q2lm <- lm(q2data$visc ~ q2data$temp)
summary(q2lm)
plot(q2lm$fitted.values, resid(q2lm))
abline(h = 0, col="red")
\end{verbatim}


\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 1.2815 & 0.0469 & 27.34 & 0.0000 \\ 
  q2data\$temp & -0.0088 & 0.0007 & -12.02 & 0.0000 \\ 
   \hline
\end{tabular}
\end{table}

Residual standard error: 0.04743 on 6 degrees of freedom

Multiple R-squared:  0.9602 \tab Adjusted R-squared:  0.9535

\simg{HW6Q2PBPlt}

\textbf{It looks like all 5 of our assumptions hold except for non constant variance for our error term (due to the quadratic pattern).}
}

\newpage

\ssc{Q2 Part (c)}{

\begin{verbatim}
q2lm <- lm(q2data$visc ~ q2data$temp + exp(q2data$temp))
summary(q2lm)
plot(q2lm$fitted.values, resid(q2lm))
plot(q2data$temp, resid(q2lm))
plot(exp(q2data$temp), resid(q2lm))
\end{verbatim}

\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 1.3195 & 0.0424 & 31.11 & 0.0000 \\ 
  q2data\$temp & -0.0096 & 0.0007 & -13.27 & 0.0000 \\ 
  exp(q2data\$temp) & 0.0000 & 0.0000 & 2.03 & 0.0986 \\ 
   \hline
\end{tabular}
\end{table}

\simg{HW6Q2PC1}
\simg{HW6Q2PC2}
\simg{HW6Q2PC3}

\textbf{Post-Transformation:}

\begin{verbatim}
q2lm_t <- lm(log(q2data$visc) ~ q2data$temp + exp(q2data$temp))
summary(q2lm_t)
plot(q2lm_t$fitted.values, resid(q2lm_t), main="[Transformed] Residuals vs Fitted", xlab="Fitted", ylab="Residuals")
plot(q2data$temp, resid(q2lm_t), main="[Transformed] Residuals vs Temp", xlab="Temp", ylab="Residuals")
plot(exp(q2data$temp), resid(q2lm_t), main="[Transformed] Residuals vs e^(temp)", xlab="e^(temp)", ylab="Residuals")
\end{verbatim}

\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 0.4036 & 0.0192 & 20.99 & 0.0000 \\ 
  q2data\$temp & -0.0121 & 0.0003 & -37.02 & 0.0000 \\ 
  exp(q2data\$temp) & 0.0000 & 0.0000 & 3.23 & 0.0232 \\ 
   \hline
\end{tabular}
\end{table}

\simg{HW6Q2PC4}
\simg{HW6Q2PC5}
\simg{HW6Q2PC6}

\textbf{Looks like we still have non-constant variance, but the p values and R$^2$ got better after both the additional \bta and the log transformation.}

}
}

\newpage

\bsc{Question 3}{
\wimg{HW6Q3}

\ssc{Q3 Part (a)}{

\simg{HW6Q3PAHist}
\simg{HW6Q3PARes}

\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 5.8945 & 4.3251 & 1.36 & 0.1783 \\ 
  x1 & -0.4779 & 0.3400 & -1.41 & 0.1653 \\ 
  x2 & 0.1827 & 0.0172 & 10.63 & 0.0000 \\ 
  x3 & 35.4028 & 11.0996 & 3.19 & 0.0023 \\ 
  x4 & 5.8439 & 2.9098 & 2.01 & 0.0494 \\ 
   \hline
\end{tabular}
\end{table}
The five key assumptions:

\balist
\item Normality - \textbf{Looks somewhat normal, maybe with a long right tail.}

\item Independence - \textbf{Since there doesn't appear to be any pattern in the residuals vs fitted values, I would say the errors are independent.}

\item Constant Variance - \textbf{By the plot in above in (b), it looks like there aren't any particular shapes in the residuals vs fitted, so the variance looks constant to me.}

\item \exv{$\epsilon$} \eql 0 - \textbf{This is assumed since that's the way we build our model (i.e. via least squares).}

\item Linearity - Multiple R-squared:  0.6914 \tab 	Adjusted R-squared:  0.6697

\textbf{\uw{x}{2} (***), \uw{x}{3} (**), and \uw{x}{4} (*) are all significant, so if \uw{H}{0} is that there is no linear relationship between any of our regressors and our response variable, then we reject \uw{H}{0}.}

\elist

}
}

\newpage

\bsc{Question 4}{
\wimg{HW6Q4}

\ssc{Q4 Part (a)}{

\simg{HW6Q4LM}

\textbf{It fits okay. Could be better. R$^2$ \eql 0.9038}

}
\ssc{Q4 Part (b)}{

\simg{HW6Q4PB}

\textbf{It suggests that, since there's a slight curve, perhaps y isn't linear.}

}
\ssc{Q4 Part (c)}{

\simg{HW6Q4PC}

\textbf{It looks like we have non constant variance (due to the quadratic shape).}
}
\ssc{Q4 Part (d)}{

\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 20.1000 & 6.3360 & 3.17 & 0.0504 \\ 
  x & -1.4696 & 0.4145 & -3.55 & 0.0382 \\ 
  I(x\verb|^|2) & 0.0598 & 0.0058 & 10.31 & 0.0019 \\ 
   \hline
\end{tabular}
\end{table}

\textbf{Yes, the x$^2$ term is statistically significant (**).}

}
\ssc{Q4 Part (e)}{

\simg{HW6Q4PE1}
\simg{HW6Q4PE2}
\simg{HW6Q4PE3}

\textbf{Yes, there is evidence it is a better fit. The non-constant variance problem seems to be reduced (although you could argue that there is still a slight quadratic shape, but there's not enough data to really say either way). The R$^2$ is also a lot better: 0.9974} 

}
}

\newpage

\bsc{Question 5}{
\wimg{HW6Q5}
\ssc{Q5 Part (a)}{

\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 33.6184 & 1.5395 & 21.84 & 0.0000 \\ 
  x1 & -0.0457 & 0.0087 & -5.27 & 0.0000 \\ 
  as.factor(x11)1 & -0.4987 & 2.2282 & -0.22 & 0.8245 \\ 
   \hline
\end{tabular}
\end{table}

\textbf{The p value of x11 in the model is 0.8245, so there isn't significant evidence to conclude that there is a linear relationship between y and x11.}

}
\ssc{Q5 Part (b)}{

\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 42.9196 & 2.7349 & 15.69 & 0.0000 \\ 
  x1 & -0.1168 & 0.0198 & -5.89 & 0.0000 \\ 
  as.factor(x11)1 & -13.4637 & 3.8441 & -3.50 & 0.0016 \\ 
  x1:as.factor(x11)1 & 0.0816 & 0.0213 & 3.84 & 0.0006 \\ 
   \hline
\end{tabular}
\end{table}

\textbf{x1 (***), x11 (**), and x1:x11(***) are significant. It looks like the type of transmission (x11) has a significant effect on the gasoline mileage (y) when you account for the interaction between the engine displacement (x1) and type of transmission (x11).}

}
}
\end{document}