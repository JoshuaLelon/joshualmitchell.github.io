% Thank you Josh Davis for this template!
% https://github.com/jdavis/latex-homework-template/blob/master/homework.tex

\documentclass{article}

\newcommand{\hmwkTitle}{HW\ \#6}

% \input{ShortcutsStatistics}

% ----------

% Packages

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}
\usepackage{enumitem}
\usepackage{chngcntr}
\usepackage{blkarray}

% Libraries

\usetikzlibrary{automata, positioning, arrows}

%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{}
\rhead{\hmwkClass\ (\hmwkClassInstructor): \hmwkTitle}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}
\setcounter{secnumdepth}{0}

\newcommand{\hmwkClass}{MATH 5345 / Regression Analysis}        % Class
\newcommand{\hmwkClassInstructor}{Dr. Sun}           % Instructor
\newcommand{\hmwkAuthorName}{\textbf{Joshua Mitchell}} % Author

%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small\vspace{0.1in}\large{\textit{\hmwkClassInstructor}}
    \vspace{3in}
}

\author{\hmwkAuthorName}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

%
% Various Helper Commands
%

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}


% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Formatting commands:

\newcommand{\mt}[1]{\ensuremath{#1}}
\newcommand{\nm}[1]{\textrm{#1}}

\newcommand\bsc[2][\DefaultOpt]{%
  \def\DefaultOpt{#2}%
  \section[#1]{#2}%
}
\newcommand\ssc[2][\DefaultOpt]{%
  \def\DefaultOpt{#2}%
  \subsection[#1]{#2}%
}
\newcommand{\bgpf}{\begin{proof} $ $\newline}

\newcommand{\bgeq}{\begin{equation*}}
\newcommand{\eeq}{\end{equation*}}	

\newcommand{\balist}{\begin{enumerate}[label=\alph*.]}
\newcommand{\elist}{\end{enumerate}}

\newcommand{\bilist}{\begin{enumerate}[label=\roman*)]}	

\newcommand{\bgsp}{\begin{split}}
% \newcommand{\esp}{\end{split}} % doesn't work for some reason.

\newcommand\prs[1]{~~~\textbf{(#1)}}

\newcommand{\lt}[1]{\textbf{Let: } #1}
     							   %  if you're setting it to be true
\newcommand{\supp}[1]{\textbf{Suppose: } #1}
     							   %  Suppose (if it'll end up false)
\newcommand{\wts}[1]{\textbf{Want to show: } #1}
     							   %  Want to show
\newcommand{\as}[1]{\textbf{Assume: } #1}
     							   %  if you think it follows from truth
\newcommand{\bpth}[1]{\textbf{(#1)}}

\newcommand{\step}[2]{\begin{equation}\tag{#2}#1\end{equation}}
\newcommand{\epf}{\end{proof}}

\newcommand{\dbs}[3]{\mt{#1_{#2_#3}}}

\newcommand{\sidenote}[1]{-----------------------------------------------------------------Side Note----------------------------------------------------------------
#1 \

---------------------------------------------------------------------------------------------------------------------------------------------}

% Analysis / Logical commands:

\newcommand{\br}{\mt{\mathbb{R}} }       % |R
\newcommand{\bq}{\mt{\mathbb{Q}} }       % |Q
\newcommand{\bn}{\mt{\mathbb{N}} }       % |N
\newcommand{\bc}{\mt{\mathbb{C}} }       % |C
\newcommand{\bz}{\mt{\mathbb{Z}} }       % |Z

\newcommand{\ep}{\mt{\epsilon} }         % epsilon
\newcommand{\fa}{\mt{\forall} }          % for all
\newcommand{\afa}{\mt{\alpha} }
\newcommand{\bta}{\mt{\beta} }
\newcommand{\mem}{\mt{\in} }
\newcommand{\exs}{\mt{\exists} }

\newcommand{\es}{\mt{\emptyset} }        % empty set
\newcommand{\sbs}{\mt{\subset} }         % subset of
\newcommand{\fs}[2]{\{\uw{#1}{1}, \uw{#1}{2}, ... \uw{#1}{#2}\}}

\newcommand{\lra}{ \mt{\longrightarrow} } % implies ----->
\newcommand{\rar}{ \mt{\Rightarrow} }     % implies -->

\newcommand{\lla}{ \mt{\longleftarrow} }  % implies <-----
\newcommand{\lar}{ \mt{\Leftarrow} }      % implies <--

\newcommand{\av}[1]{\mt{|}#1\mt{|}}  % absolute value

\newcommand{\prn}[1]{(#1)}
\newcommand{\bk}[1]{\{#1\}}

\newcommand{\ps}{\mt{\operatorname{+}} }
\newcommand{\ms}{\mt{\operatorname{-}} }

\newcommand{\ls}{\mt{\operatorname{<}} }
\newcommand{\gr}{\mt{\operatorname{>}} }

\newcommand{\lse}{\mt{\operatorname{\leq}} }
\newcommand{\gre}{\mt{\operatorname{\geq}} }

\newcommand{\eql}{ \mt{\operatorname{=}} }

\newcommand{\pr}{\mt{^\prime}} 		   % prime (i.e. R')
\newcommand{\uw}[2]{#1\mt{_{#2}}}
\newcommand{\uf}[2]{#1\mt{^{#2}}}
\newcommand{\frc}[2]{\mt{\frac{#1}{#2}}}
\newcommand{\lmti}[1]{\mt{\displaystyle{\lim_{#1 \to \infty}}}}
\newcommand{\limt}[2]{\mt{\displaystyle{\lim_{#1 \to #2}}}}

\newcommand{\bnm}[2]{\mt{#1\setminus{#2}}}
\newcommand{\bnt}[2]{\mt{\textrm{#1}\setminus{\textrm{#2}}}}
\newcommand{\bi}{\bnm{\mathbb{R}}{\mathbb{Q}}}

\newcommand{\urng}[2]{\mt{\bigcup_{#1}^{#2}}}
\newcommand{\nrng}[2]{\mt{\bigcap_{#1}^{#2}}}
\newcommand{\nck}[2]{\mt{{#1 \choose #2}}}

\newcommand{\nbho}[3]{\textrm{N(}#1, #2\textrm{) }\cap \textrm{ #3} \neq \emptyset}
     							   %  N(x, eps) intersect S \= emptyset
\newcommand{\nbhe}[3]{\textrm{N(}#1, #2\textrm{) }\cap \textrm{ #3} = \emptyset}
     							   %  N(x, eps) intersect S  = emptyset
\newcommand{\dnbho}[3]{\textrm{N*(}#1, #2\textrm{) }\cap \textrm{ #3} \neq \emptyset}
     							   %  N*(x, eps) intersect S \= emptyset
\newcommand{\dnbhe}[3]{\textrm{N*(}#1, #2\textrm{) }\cap \textrm{ #3} = \emptyset}
     							   %  N*(x, eps) intersect S = emptyset
     							   
\newcommand{\eqn}[1]{\[#1\]}
\newcommand{\splt}[1]{\begin{split}#1\end{split}}

\newcommand{\txt}[1]{\text{#1}} % Not new command, but remember \text for text in eqns
\newcommand{\tl}{\mt{\thicksim}}
\newcommand{\mn}[1]{\mt{\overline{#1}}}
\newcommand{\sg}{\mt{\sigma}}
\newcommand{\ssq}{\mt{\sigma^2}}	

\newcommand{\bh}[1]{\mathbf{\hat{\text{$#1$}}}}
\newcommand{\bth}{\mt{\bh{\beta}}}
\newcommand{\yh}{\mt{\bh{Y}}}

\newcommand{\exv}[1]{\txt{E[}#1\txt{]}}
\newcommand{\vrn}[1]{V[#1]}

\newcommand{\gv}{ \mt{|} }

\newcommand{\cov}[2]{\txt{Cov(#1, #2)}}

\newcommand{\img}[1]{
\begin{figure}[h]
  \includegraphics[width=0.5\linewidth]{#1}
\end{figure}
}
\newcommand{\simg}[1]{
  \includegraphics[width=0.35\linewidth]{#1}
}
\newcommand{\wimg}[1]{
\begin{figure}[h]
  \includegraphics[width=1\linewidth]{#1}
\end{figure}
}
\newcommand\tab[1][1cm]{\hspace*{#1}}	
\newcommand{\sumin}[1]{\mt{\sum_{i = 1}^n #1}}	

\newcommand{\brm}[1]{\begin{pmatrix} #1 \end{pmatrix}}

\newcommand{\inm}[1]{\mt{\left\[ \begin{smallmatrix} #1 \end{smallmatrix} \right\]}}

\newcommand{\lbm}[4]{
	  \begin{blockarray}{#1} % a c for every row, plus the c-labels
        #2 \\ % & c-label1 & c-label2 & c-label3...
      \begin{block}{c(#3)} % a c for every column only
        #4 % r-label1 & data & data & data ... \\
           % r-label2 & data & data & data ... \\
           % ...
           % r-labeln & data & data & data ... \\
      \end{block}
    \end{blockarray}
}

% Example:
% \[\lbm{ccccc}{& H & y & d}{cccc}{H & 4 & 4 & 4 \\
% Y & 3 & 3 & 3 \\
% D & 2 & 2 & 2 \\
% D & 2 & 2 & 2 \\}\]

\newcommand{\unds}[2]{\mt{\underset{#1}{#2}}} % stuff underneath!	 
% ----------

\begin{document}
\begin{table}[ht]
\centering
\begin{tabular}{llllll}
  \hline
 & Q1 & Q2 & Q3 & Q4 & Q5 \\ 
  \hline
50 Points & 10 & 10 & 10 & 10 & 10 \\ 
   &  &  &  &  &  \\ 
   \hline
\end{tabular}
\end{table}
\bsc{Question 1}{
\wimg{HW6Q1}
\ssc{Q1 Part (a)}{

The five key assumptions:

\balist
\item Normality - Our response variable(s) (by themselves), residuals (by themselves), and residuals vs regressors, when histogrammed, look normal.

\simg{HW6Q1A1}

\textbf{The distribution of our response variable seems mostly normal - I don't know what to make of the gap in the middle, though.}

\item Independence - Our samples are independent (i.e. the value of one does not affect the value of any other). This is usually the hardest one to test for - usually it's argued from a sampling side. If you plot the residuals vs the predicted values, if they're independent, you should see no pattern.

\simg{HW6Q1X1}
\simg{HW6Q1X2}
\simg{HW6Q1X3}

\textbf{Our samples appear to be independent - there doesn't seem to be a pattern in the data (but then again, there's only 13 data points).}

\item Constant Variance - The residual plots should just be bands (i.e. no funnels, cones, or any weird shape).

\textbf{It does appear that we have a "bowtie" kind of pattern, so I would assume that we don't have constant variance.}

\item \exv{$\epsilon$} \eql 0 - This is assumed since that's the way we build our model (i.e. via least squares)

\textbf{Since we made our model this same way, I think it's valid to say we have \exv{$\epsilon$} \eql 0.}

\item Linearity - The model actually fits (i.e. the data follows the shape of the model: R$^2$ is high)

\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 27.7692 & 5.3190 & 5.22 & 0.0008 \\ 
  as.factor(x1)0 & 0.7308 & 6.5577 & 0.11 & 0.9140 \\ 
  as.factor(x1)1 & 31.8462 & 6.7280 & 4.73 & 0.0015 \\ 
  as.factor(x2)1 & -8.4615 & 6.2935 & -1.34 & 0.2157 \\ 
  as.factor(x3)1 & -9.9615 & 6.2935 & -1.58 & 0.1521 \\ 
   \hline
\end{tabular}
\end{table}

Multiple R-squared:  0.7393, 	Adjusted R-squared:  0.609

F-statistic: 5.672 on 4 and 8 DF,  p-value: 0.01828

Only the intercept (***) and as.factor(x1)0 (**) are significant.

\textbf{I would say the model somewhat fits - it's hard to say since we only have 13 data points. Our R$^2$ is mediocre, and we have a lot of degrees of freedom relative to our number of samples.}

\textbf{Technically, if we do a hypothesis test and assume \uw{H}{0}: all \bta's are 0, then we can disprove the null hypothesis with our \bta for \uw{x}{1}'s 2nd factor (as shown in the table: as.factor(x1)1). But, everything else is insignificant. I'd say the model fits, but barely (needs changes).}

\elist

}
\ssc{Q1 Part (b)}{

Data:

\simg{HW6Q1Data}

Recall:

\uw{SS}{Res} \eql \uw{SS}{PE} \ps \uw{SS}{LOF}

Our test statistic is:
\eqn{F_0 = \frac{SS_{LOF} / (m - 2)}{SS_{PE} / (n - m)} = \frac{MS_{LOF}}{MS_{PE}}}

If \uw{F}{0} \gr \uw{F}{m \ms 2, n \ms m}, conclude that the regression function is not linear.

From R:

\begin{table}[ht]
\centering
\begin{tabular}{lrrrrr}
  \hline
 & Df & Sum Sq & Mean Sq & F value & Pr($>$F) \\ 
  \hline
as.factor(x1) & 2 & 1372.44 & 686.22 & 9.33 & 0.0081 \\ 
  as.factor(x2) & 1 & 112.12 & 112.12 & 1.52 & 0.2520 \\ 
  as.factor(x3) & 1 & 184.29 & 184.29 & 2.51 & 0.1521 \\ 
  Residuals & 8 & 588.46 & 73.56 &  &  \\ 
   \hline
\end{tabular}
\end{table}

Since we have 5 levels of (\uw{x}{1} \eql 0, \uw{x}{2} \eql 0, \uw{x}{3} \eql 0),
\eqn{m = 5}
\eqn{(29 + 28.5 + 30 + 27 + 28) / 5 = 28.5 = \bar y_i}
\eqn{(29 - 28.5)^2 + (28.5 - 28.5)^2 + (30 - 28.5)^2 + (27 - 28.5)^2 + (28 - 28.5)^2 \eql 5 = SS_{PE}}
\eqn{588.46 = 5 + SS_{LOF} \lra SS_{LOF} = 583.46}
\eqn{F_0 = \frac{583.46 / (5 - 2)}{5 / (13 - 5)} = \frac{194.486666667}{0.625} = 311.178666667}

Test:

\eqn{311.178666667 > \uw{F}{m \ms 2, n \ms m} = F_{3, 8} = 2.92380 \textrm{ (even at \afa \eql 0.10 it still fails)}}

\textbf{So we conclude that the regression function is not linear.}

}

}

\newpage

\bsc{Question 2}{
\wimg{HW6Q2}
\ssc{Q2 Part (a)}{
\simg{HW6Q2ScPlt}

\textbf{Yes, it seems likely that a straight line model will be adequate.}
}
\ssc{Q2 Part (b)}{

\begin{verbatim}
q2lm <- lm(q2data$visc ~ q2data$temp)
summary(q2lm)
plot(q2lm$fitted.values, resid(q2lm))
abline(h = 0, col="red")
\end{verbatim}


\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 1.2815 & 0.0469 & 27.34 & 0.0000 \\ 
  q2data\$temp & -0.0088 & 0.0007 & -12.02 & 0.0000 \\ 
   \hline
\end{tabular}
\end{table}

Residual standard error: 0.04743 on 6 degrees of freedom

Multiple R-squared:  0.9602 \tab Adjusted R-squared:  0.9535

\simg{HW6Q2PBPlt}

\textbf{It looks like all 5 of our assumptions hold except for non constant variance for our error term (due to the quadratic pattern).}
}

\newpage

\ssc{Q2 Part (c)}{

\begin{verbatim}
q2lm <- lm(q2data$visc ~ q2data$temp + exp(q2data$temp))
summary(q2lm)
plot(q2lm$fitted.values, resid(q2lm))
plot(q2data$temp, resid(q2lm))
plot(exp(q2data$temp), resid(q2lm))
\end{verbatim}

\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 1.3195 & 0.0424 & 31.11 & 0.0000 \\ 
  q2data\$temp & -0.0096 & 0.0007 & -13.27 & 0.0000 \\ 
  exp(q2data\$temp) & 0.0000 & 0.0000 & 2.03 & 0.0986 \\ 
   \hline
\end{tabular}
\end{table}

\simg{HW6Q2PC1}
\simg{HW6Q2PC2}
\simg{HW6Q2PC3}

\textbf{Post-Transformation:}

\begin{verbatim}
q2lm_t <- lm(log(q2data$visc) ~ q2data$temp + exp(q2data$temp))
summary(q2lm_t)
plot(q2lm_t$fitted.values, resid(q2lm_t), main="[Transformed] Residuals vs Fitted", xlab="Fitted", ylab="Residuals")
plot(q2data$temp, resid(q2lm_t), main="[Transformed] Residuals vs Temp", xlab="Temp", ylab="Residuals")
plot(exp(q2data$temp), resid(q2lm_t), main="[Transformed] Residuals vs e^(temp)", xlab="e^(temp)", ylab="Residuals")
\end{verbatim}

\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 0.4036 & 0.0192 & 20.99 & 0.0000 \\ 
  q2data\$temp & -0.0121 & 0.0003 & -37.02 & 0.0000 \\ 
  exp(q2data\$temp) & 0.0000 & 0.0000 & 3.23 & 0.0232 \\ 
   \hline
\end{tabular}
\end{table}

\simg{HW6Q2PC4}
\simg{HW6Q2PC5}
\simg{HW6Q2PC6}

}

}

\bsc{Question 3}{
\wimg{HW6Q3}
}

\bsc{Question 4}{
\wimg{HW6Q4}
}
\bsc{Question 5}{
\wimg{HW6Q5}
}
\end{document}